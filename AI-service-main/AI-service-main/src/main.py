import re
import uuid
import time
import json
import logging
from contextlib import asynccontextmanager
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from src.configs.model_init import chat_model
from langgraph.checkpoint.memory import MemorySaver
from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse, StreamingResponse
import uvicorn
from src.schemas.model_response import Choice, ModelResponseData, ModelResponse
from src.prompts.prompts import user_prompt, greeting_card_prompt,test_prompt
from src.utils.excel_processor import excel_processor
from datetime import datetime
import threading
import sys
import io
import unicodedata

# ç”³æ˜å…¨å±€å˜é‡ å…¨å±€è°ƒç”¨
graph = None
field_mapping_graph = None  # æ–°å¢ï¼šå­—æ®µæ˜ å°„å›¾
# è·å–å½“å‰æ—¶é—´å¹¶æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²ï¼ˆä¾‹å¦‚ï¼š2023-11-15 14:30:45ï¼‰
current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

import os
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "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"
os.environ["LANGCHAIN_PROJECT"] = "AI-service-field-mapping"  # é¡¹ç›®åç§°

# è®¾ç½®æ—¥å¿—æ¨¡ç‰ˆ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', handlers=[logging.StreamHandler(sys.stdout)])
logger = logging.getLogger(__name__)
from src.configs.settings import manager, RegisterConfig
service_select = manager.get_service_config("greet-system")
app_host = service_select.app_host
app_port  = int(service_select.app_port)



class Message(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    # messages: List[Message]
    # messages: str
    # user_input: str
    userInput: str
    stream: Optional[bool] = True
    userId: Optional[str] = None
    conversationId: Optional[str] = None
    excelData: Optional[str] = None  # JSONæ ¼å¼çš„Excelæ•°æ®
    excelFilename: Optional[str] = None  # Excelæ–‡ä»¶å



class ChatCompletionResponseChoice(BaseModel):
    index: int
    message: Message
    finish_reason: Optional[str] = None

class ChatCompletionResponse(BaseModel):
    id: str = Field(default_factory=lambda: f"chatcmpl-{uuid.uuid4().hex}")
    object: str = "chat.completion"
    created: int = Field(default_factory=lambda: int(time.time()))
    choices: List[ChatCompletionResponseChoice]
    system_fingerprint: Optional[str] = None

class State(TypedDict):
    messages: Annotated[list, add_messages]

# æ–°å¢ï¼šå­—æ®µæ˜ å°„çŠ¶æ€ç±»
class FieldMappingState(TypedDict):
    excel_data: Dict[str, Any]  # ExcelåŸå§‹æ•°æ®
    preprocessed_fields: List[Dict[str, Any]]  # é¢„å¤„ç†åçš„å­—æ®µ
    classified_fields: List[Dict[str, Any]]  # åˆ†ç±»åçš„å­—æ®µ
    initial_mapping: Dict[str, str]  # åˆæ­¥æ˜ å°„ç»“æœ
    final_mapping: Dict[str, str]  # æœ€ç»ˆæ˜ å°„ç»“æœ
    validation_results: Dict[str, Any]  # éªŒè¯ç»“æœ
    confidence_score: float  # å‡†ç¡®ç‡è¯„åˆ†
    iteration_count: int  # è¿­ä»£æ¬¡æ•°
    errors: List[str]  # é”™è¯¯ä¿¡æ¯
    messages: Annotated[list, add_messages]  # æ¶ˆæ¯å†å²

# æ–°å¢ï¼šå¢å¼ºçš„LLMå“åº”è§£æå™¨
def parse_llm_response_robust(result_content: str) -> Dict[str, str]:
    """å¢å¼ºçš„LLMå“åº”è§£æå™¨ï¼Œèƒ½å¤Ÿå¤„ç†å¤šç§æ ¼å¼çš„å“åº”"""
    try:
        import re
        
        # æ ‡å‡†å­—æ®µåˆ—è¡¨
        standard_fields = [
            "è¿è¾“æ—¥æœŸ", "è®¢å•å·", "è·¯é¡º", "æ‰¿è¿å•†", "è¿å•å·", "è½¦å‹",
            "å‘è´§æ–¹ç¼–å·", "å‘è´§æ–¹åç§°", "æ”¶è´§æ–¹ç¼–ç ", "æ”¶è´§æ–¹åç§°",
            "å•†å“ç¼–ç ", "å•†å“åç§°", "ä»¶æ•°", "ä½“ç§¯", "é‡é‡"
        ]
        
        # å°è¯•å¤šç§æ–¹å¼æå–JSON
        json_patterns = [
            r'\{.*\}',  # æ ‡å‡†JSON
            r'```json\s*(\{.*?\})\s*```',  # Markdownä»£ç å—
            r'```\s*(\{.*?\})\s*```',  # é€šç”¨ä»£ç å—
        ]
        
        for pattern in json_patterns:
            json_match = re.search(pattern, result_content, re.DOTALL)
            if json_match:
                try:
                    json_str = json_match.group(1) if len(json_match.groups()) > 0 else json_match.group(0)
                    llm_result = json.loads(json_str)
                    
                    # å¤„ç†ä¸åŒçš„è¿”å›æ ¼å¼
                    if isinstance(llm_result, dict):
                        if "æ˜ å°„ç»“æœ" in llm_result and isinstance(llm_result["æ˜ å°„ç»“æœ"], dict):
                            # æ ¼å¼1: {"æ˜ å°„ç»“æœ": {...}}
                            return llm_result["æ˜ å°„ç»“æœ"]
                        elif "mapping" in llm_result and isinstance(llm_result["mapping"], dict):
                            # æ ¼å¼2: {"mapping": {...}}
                            return llm_result["mapping"]
                        elif any(k in llm_result for k in standard_fields):
                            # æ ¼å¼3: ç›´æ¥å­—æ®µæ˜ å°„ {"è¿è¾“æ—¥æœŸ": "å­—æ®µå", ...}
                            return {k: llm_result.get(k, "missing") for k in standard_fields}
                        elif "confidence" in llm_result or "å‡†ç¡®ç‡" in llm_result:
                            # æ ¼å¼4: åŒ…å«ç½®ä¿¡åº¦çš„æ˜ å°„
                            # å°è¯•ä»å“åº”ä¸­æå–å­—æ®µæ˜ å°„ä¿¡æ¯
                            return extract_mapping_from_text(result_content, standard_fields)
                    
                except (json.JSONDecodeError, KeyError):
                    continue
        
        # å¦‚æœæ‰€æœ‰JSONè§£æéƒ½å¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–
        return extract_mapping_from_text(result_content, standard_fields)
        
    except Exception as e:
        logger.error(f"LLMå“åº”è§£æå¤±è´¥: {str(e)}")
        return {}
    
def apply_similarity_fill(initial_mapping: dict, preprocessed_fields: list, threshold: float = 0.4, only_missing: bool = True) -> dict:
    import re, unicodedata
    def _norm(s: str) -> str:
        if s is None: return ""
        t = unicodedata.normalize("NFKC", str(s)).lower()
        t = re.sub(r"[_\-\./\\]+", " ", t)
        return re.sub(r"\s+", " ", t).strip()
    def _tok(s: str):
        return re.findall(r"[a-z0-9]+|[\u4e00-\u9fa5]+", _norm(s))
    def _edit_sim(a: str, b: str) -> float:
        a, b = _norm(a), _norm(b)
        if not a and not b: return 1.0
        if not a or not b: return 0.0
        la, lb = len(a), len(b)
        dp = [[0]*(lb+1) for _ in range(la+1)]
        for i in range(la+1): dp[i][0] = i
        for j in range(lb+1): dp[0][j] = j
        for i in range(1, la+1):
            for j in range(1, lb+1):
                dp[i][j] = min(dp[i-1][j]+1, dp[i][j-1]+1, dp[i-1][j-1]+(a[i-1]!=b[j-1]))
        dist = dp[la][lb]
        return 1 - dist / max(1, max(la, lb))
    def _jaccard(a: str, b: str) -> float:
        sa, sb = set(_tok(a)), set(_tok(b))
        return (len(sa & sb) / len(sa | sb)) if sa and sb else 0.0
    UNIT_HINTS = {"ä½“ç§¯": ["cbm","m3","mÂ³","ç«‹æ–¹","å®¹ç§¯","cubic","vol"], "é‡é‡": ["kg","å…¬æ–¤","åƒå…‹","å¨","lb","wt","weight"], "ä»¶æ•°": ["pcs","box","qty","count","ç®±","åŒ…","æ•°é‡","ä»¶"]}
    TOTAL_HINTS = ["æ€»","åˆè®¡","æ€»è®¡","total","sum"]
    def _bonus(name: str, field: str) -> float:
        n = _norm(name); b = 0.0
        if field in UNIT_HINTS and any(h in n for h in UNIT_HINTS[field]): b += 0.2
        if field in ["ä»¶æ•°","ä½“ç§¯","é‡é‡"] and any(h in n for h in TOTAL_HINTS): b += 0.2
        return min(b, 0.4)

    std_fields = ["è¿è¾“æ—¥æœŸ","è®¢å•å·","è·¯é¡º","æ‰¿è¿å•†","è¿å•å·","è½¦å‹","å‘è´§æ–¹ç¼–å·","å‘è´§æ–¹åç§°","æ”¶è´§æ–¹ç¼–ç ","æ”¶è´§æ–¹åç§°","å•†å“ç¼–ç ","å•†å“åç§°","ä»¶æ•°","ä½“ç§¯","é‡é‡"]
    if not preprocessed_fields: return initial_mapping
    cols = [f.get("name") for f in preprocessed_fields if f.get("name")]
    used = set(v for v in initial_mapping.values() if v and v != "missing")
    updated = dict(initial_mapping)

    for f in std_fields:
        if only_missing and updated.get(f) and updated[f] != "missing":
            continue
        best_name, best_score = None, 0.0
        for name in cols:
            if name in used: continue
            score = 0.6*_edit_sim(name, f) + 0.4*_jaccard(name, f) + _bonus(name, f)
            if score > best_score:
                best_score, best_name = score, name
        if best_name and best_score >= threshold:
            updated[f] = best_name
            used.add(best_name)
    return updated

def extract_mapping_from_text(text: str, standard_fields: List[str]) -> Dict[str, str]:
    """ä»æ–‡æœ¬ä¸­æå–å­—æ®µæ˜ å°„ä¿¡æ¯"""
    try:
        mapping = {field: "missing" for field in standard_fields}
        
        # å°è¯•ä»æ–‡æœ¬ä¸­è¯†åˆ«å­—æ®µæ˜ å°„
        for field in standard_fields:
            # æŸ¥æ‰¾åŒ…å«å­—æ®µåçš„è¡Œ
            field_pattern = rf'{field}[ï¼š:]\s*["""]?([^"""\n]+)["""]?'
            match = re.search(field_pattern, text)
            if match:
                mapped_field = match.group(1).strip()
                if mapped_field and mapped_field != "missing":
                    mapping[field] = mapped_field
        
        return mapping
        
    except Exception as e:
        logger.error(f"æ–‡æœ¬æå–æ˜ å°„å¤±è´¥: {str(e)}")
        return {field: "missing" for field in standard_fields}

def try_hybrid_mapping(state: FieldMappingState, llm_response: str) -> Dict[str, str]:
    """æ··åˆç­–ç•¥ï¼šç»“åˆLLMå“åº”å’Œè§„åˆ™æ˜ å°„"""
    try:
        # æ ‡å‡†å­—æ®µåˆ—è¡¨
        standard_fields = [
            "è¿è¾“æ—¥æœŸ", "è®¢å•å·", "è·¯é¡º", "æ‰¿è¿å•†", "è¿å•å·", "è½¦å‹",
            "å‘è´§æ–¹ç¼–å·", "å‘è´§æ–¹åç§°", "æ”¶è´§æ–¹ç¼–ç ", "æ”¶è´§æ–¹åç§°",
            "å•†å“ç¼–ç ", "å•†å“åç§°", "ä»¶æ•°", "ä½“ç§¯", "é‡é‡"
        ]
        
        # åˆå§‹åŒ–æ˜ å°„
        hybrid_mapping = {field: "missing" for field in standard_fields}
        
        # å°è¯•ä»LLMå“åº”ä¸­æå–éƒ¨åˆ†ä¿¡æ¯
        partial_mapping = extract_mapping_from_text(llm_response, standard_fields)
        
        # è·å–è§„åˆ™æ˜ å°„ä½œä¸ºè¡¥å……
        rule_mapping = get_rule_based_mapping(state)
        
        # åˆå¹¶æ˜ å°„ï¼šä¼˜å…ˆä½¿ç”¨LLMç»“æœï¼Œè§„åˆ™æ˜ å°„ä½œä¸ºè¡¥å……
        for field in standard_fields:
            if partial_mapping.get(field) != "missing":
                hybrid_mapping[field] = partial_mapping[field]
            elif rule_mapping.get(field) != "missing":
                hybrid_mapping[field] = rule_mapping[field]
        
        logger.info(f"ğŸ” [æ··åˆç­–ç•¥] æˆåŠŸæ˜ å°„ {len([v for v in hybrid_mapping.values() if v != 'missing'])} ä¸ªå­—æ®µ")
        return hybrid_mapping
        
    except Exception as e:
        logger.error(f"æ··åˆç­–ç•¥å¤±è´¥: {str(e)}")
        return {field: "missing" for field in standard_fields}

def get_rule_based_mapping(state: FieldMappingState) -> Dict[str, str]:
    """è·å–åŸºäºè§„åˆ™çš„æ˜ å°„"""
    try:
        classified_fields = state.get("classified_fields", [])
        standard_fields = [
            "è¿è¾“æ—¥æœŸ", "è®¢å•å·", "è·¯é¡º", "æ‰¿è¿å•†", "è¿å•å·", "è½¦å‹",
            "å‘è´§æ–¹ç¼–å·", "å‘è´§æ–¹åç§°", "æ”¶è´§æ–¹ç¼–ç ", "æ”¶è´§æ–¹åç§°",
            "å•†å“ç¼–ç ", "å•†å“åç§°", "ä»¶æ•°", "ä½“ç§¯", "é‡é‡"
        ]
        
        rule_mapping = {}
        used_fields = set()
        
        for standard_field in standard_fields:
            best_match = "missing"
            best_score = 0
            
            for field in classified_fields:
                if field["name"] in used_fields:
                    continue
                    
                score = calculate_mapping_score(standard_field, field)
                if score > best_score:
                    best_score = score
                    best_match = field["name"]
            
            if best_score > 0.3:
                rule_mapping[standard_field] = best_match
                used_fields.add(best_match)
            else:
                rule_mapping[standard_field] = "missing"
        
        return rule_mapping
        
    except Exception as e:
        logger.error(f"è§„åˆ™æ˜ å°„è·å–å¤±è´¥: {str(e)}")
        return {field: "missing" for field in standard_fields}

# æ–°å¢ï¼šå­—æ®µé¢„å¤„ç†èŠ‚ç‚¹
def preprocess_fields(state: FieldMappingState) -> FieldMappingState:
    """å­—æ®µé¢„å¤„ç†ï¼šæ¸…æ´—è„æ•°æ®ï¼Œæå–å­—æ®µç‰¹å¾"""
    try:
        # æ·»åŠ LangSmithè¿½è¸ªæ ‡ç­¾
        from langsmith import traceable
        import uuid
        
        # åˆ›å»ºè¿½è¸ªID
        trace_id = str(uuid.uuid4())
        
        excel_data = state["excel_data"]
        preprocessed_fields = []
        
        # ç¡®ä¿excel_dataæ˜¯å­—å…¸æ ¼å¼ï¼Œå¦‚æœæ˜¯åˆ—è¡¨åˆ™è½¬æ¢ä¸ºå­—å…¸
        if isinstance(excel_data, list) and len(excel_data) > 0:
            # Excelå¤„ç†å™¨è¿”å›çš„æ˜¯è¡Œè®°å½•åˆ—è¡¨ï¼Œéœ€è¦è½¬æ¢ä¸ºåˆ—æ•°æ®æ ¼å¼
            if isinstance(excel_data[0], dict):
                # è½¬æ¢ä¸ºåˆ—å:åˆ—æ•°æ®çš„æ ¼å¼
                columns = list(excel_data[0].keys())
                converted_data = {}
                for col in columns:
                    converted_data[col] = [row.get(col) for row in excel_data]
                excel_data = converted_data
        
        for column_name, column_data in excel_data.items():
            if not isinstance(column_data, list):
                continue
                
            # è®¡ç®—å­—æ®µç‰¹å¾
            total_rows = len(column_data)
            non_null_count = sum(1 for value in column_data if value is not None and value != "")
            null_rate = (total_rows - non_null_count) / total_rows if total_rows > 0 else 1.0
            
            # å‘è´§æ–¹ç›¸å…³å­—æ®µçš„ç™½åå•ä¿æŠ¤ï¼ˆå³ä½¿ç©ºå€¼ç‡é«˜ä¹Ÿè¦ä¿ç•™ï¼‰
            shipper_keywords = [
                "ç«™ç‚¹", "ç½‘ç‚¹", "é—¨åº—", "ä»“åº“", "é…é€ä¸­å¿ƒ", "DC", "å·¥å‚",
                "ç«™å", "ç½‘å", "åº—å", "ä»“å", "å‚å",
                "ç«™å·", "ç½‘å·", "åº—å·", "ä»“å·", "å‚å·",
                "ç«™ç¼–ç ", "ç½‘ç¼–ç ", "åº—ç¼–ç ", "ä»“ç¼–ç ", "å‚ç¼–ç ",
                "å‘è´§", "å‘é€", "å§‹å‘", "èµ·è¿", "å‘è¿"
            ]
            
            is_shipper_field = any(keyword in column_name for keyword in shipper_keywords)
            
            # è¿‡æ»¤æ¡ä»¶ï¼šç©ºå€¼ç‡>90%ã€å…¨0ã€å…¨1çš„åˆ—ï¼ˆå‘è´§æ–¹å­—æ®µé™¤å¤–ï¼‰
            if null_rate > 0.9 and not is_shipper_field:
                if is_shipper_field:
                    logger.info(f"ğŸ” [é¢„å¤„ç†] ä¿æŠ¤å‘è´§æ–¹å­—æ®µ: {column_name} (ç©ºå€¼ç‡: {null_rate:.2%})")
                continue
            elif is_shipper_field and null_rate > 0.9:
                logger.info(f"ğŸ” [é¢„å¤„ç†] å‘è´§æ–¹å­—æ®µè¢«ä¿æŠ¤: {column_name} (ç©ºå€¼ç‡: {null_rate:.2%})")
                
            # è®¡ç®—å”¯ä¸€æ€§
            unique_values = set(str(v) for v in column_data if v is not None and v != "")
            uniqueness = len(unique_values) / total_rows if total_rows > 0 else 0
            
            # æ£€æµ‹æ•°æ®ç±»å‹
            data_type = "text"
            if all(isinstance(v, (int, float)) for v in column_data if v is not None):
                data_type = "numeric"
            elif all(isinstance(v, str) and len(v) == 10 for v in column_data if v is not None):
                # ç®€å•æ—¥æœŸæ£€æµ‹
                data_type = "date"
            
            field_info = {
                "name": column_name,
                "data": column_data,
                "null_rate": null_rate,
                "uniqueness": uniqueness,
                "data_type": data_type,
                "total_rows": total_rows,
                "non_null_count": non_null_count
            }
            preprocessed_fields.append(field_info)
        
        state["preprocessed_fields"] = preprocessed_fields
        state["messages"].append({"role": "system", "content": f"å­—æ®µé¢„å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(preprocessed_fields)} ä¸ªæœ‰æ•ˆå­—æ®µ"})
        
        # è®°å½•LangSmithè¿½è¸ªä¿¡æ¯
        logger.info(f"ğŸ” [LangSmith] èŠ‚ç‚¹: preprocess_fields | è¿½è¸ªID: {trace_id} | å¤„ç†å­—æ®µæ•°: {len(preprocessed_fields)}")
        
        return state
        
    except Exception as e:
        state["errors"].append(f"å­—æ®µé¢„å¤„ç†å¤±è´¥: {str(e)}")
        return state

# æ–°å¢ï¼šå­—æ®µåˆ†ç±»èŠ‚ç‚¹
def classify_fields(state: FieldMappingState) -> FieldMappingState:
    """åˆæ­¥å­—æ®µåˆ†ç±»ï¼šå°†å­—æ®µåˆ†é…åˆ°å€™é€‰ç±»åˆ«"""
    try:
        # æ·»åŠ LangSmithè¿½è¸ªæ ‡ç­¾
        from langsmith import traceable
        import uuid
        
        # åˆ›å»ºè¿½è¸ªID
        trace_id = str(uuid.uuid4())
        
        preprocessed_fields = state["preprocessed_fields"]
        classified_fields = []
        
        for field in preprocessed_fields:
            field_name = field["name"].lower()
            data_type = field["data_type"]
            uniqueness = field["uniqueness"]
            
            # å¯å‘å¼è§„åˆ™åˆ†ç±»
            category = "unknown"
            
            # æ—¥æœŸç±»æ£€æµ‹
            if any(keyword in field_name for keyword in ["æ—¶é—´", "æ—¥æœŸ", "date", "time"]) or data_type == "date":
                category = "date"
            # ç¼–å·ç±»æ£€æµ‹
            elif any(keyword in field_name for keyword in ["ç¼–å·", "ç¼–ç ", "code", "id", "å·"]) and uniqueness > 0.8:
                category = "identifier"
            # æ•°é‡ç±»æ£€æµ‹
            elif any(keyword in field_name for keyword in ["æ•°é‡", "ä»¶æ•°", "ä½“ç§¯", "é‡é‡", "count", "volume", "weight"]) and data_type == "numeric":
                category = "quantity"
            # åç§°ç±»æ£€æµ‹
            elif any(keyword in field_name for keyword in ["åç§°", "name", "åœ°å€", "address", "å…¬å¸", "company"]):
                category = "name"
            
            field["category"] = category
            classified_fields.append(field)
        
        state["classified_fields"] = classified_fields
        state["messages"].append({"role": "system", "content": f"å­—æ®µåˆ†ç±»å®Œæˆï¼Œå…±åˆ†ç±» {len(classified_fields)} ä¸ªå­—æ®µ"})
        
        # è®°å½•LangSmithè¿½è¸ªä¿¡æ¯
        logger.info(f"ğŸ·ï¸ [LangSmith] èŠ‚ç‚¹: classify_fields | è¿½è¸ªID: {trace_id} | åˆ†ç±»å­—æ®µæ•°: {len(classified_fields)}")
        
        return state
        
    except Exception as e:
        state["errors"].append(f"å­—æ®µåˆ†ç±»å¤±è´¥: {str(e)}")
        return state

# æ–°å¢ï¼šLLMæ™ºèƒ½å­—æ®µåŒ¹é…èŠ‚ç‚¹
def llm_map_to_standard_fields(state: FieldMappingState) -> FieldMappingState:
    """ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½å­—æ®µåŒ¹é…"""
    try:
        from src.prompts.prompts import test_prompt
        from src.configs.model_init import chat_model
        
        # æ·»åŠ LangSmithè¿½è¸ªæ ‡ç­¾
        from langsmith import traceable
        import uuid
        
        # åˆ›å»ºè¿½è¸ªID
        trace_id = str(uuid.uuid4())
        logger.info(f"ğŸ§  [LangSmith] å¼€å§‹LLMæ™ºèƒ½å­—æ®µæ˜ å°„ | è¿½è¸ªID: {trace_id}")
        
        # å‡†å¤‡Excelæ•°æ®ä¾›LLMåˆ†æ
        excel_data = state["excel_data"]
        
        # æ·»åŠ è°ƒè¯•æ—¥å¿— - æ˜¾ç¤ºLLMæ”¶åˆ°çš„å­—æ®µä¿¡æ¯
        logger.info(f"ğŸ” [LLM] excel_dataç±»å‹: {type(excel_data)}")
        logger.info(f"ğŸ” [LLM] excel_dataé•¿åº¦: {len(excel_data) if hasattr(excel_data, '__len__') else 'N/A'}")
        
        if isinstance(excel_data, dict):
            field_names = list(excel_data.keys())
            logger.info(f"ğŸ” [LLM] å‡†å¤‡å‘é€ç»™LLMçš„å­—æ®µ: {field_names}")
            logger.info(f"ğŸ” [LLM] å­—æ®µæ•°é‡: {len(field_names)}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å‘è´§æ–¹ç›¸å…³çš„å­—æ®µ
            shipper_keywords = [
                "ç«™ç‚¹", "ç½‘ç‚¹", "é—¨åº—", "ä»“åº“", "é…é€ä¸­å¿ƒ", "DC", "å·¥å‚",
                "ç«™å", "ç½‘å", "åº—å", "ä»“å", "å‚å",
                "ç«™å·", "ç½‘å·", "åº—å·", "ä»“å·", "å‚å·",
                "ç«™ç¼–ç ", "ç½‘ç¼–ç ", "åº—ç¼–ç ", "ä»“ç¼–ç ", "å‚ç¼–ç ",
                "å‘è´§", "å‘é€", "å§‹å‘", "èµ·è¿", "å‘è¿"
            ]
            
            shipper_fields = []
            for field in field_names:
                if any(keyword in field for keyword in shipper_keywords):
                    shipper_fields.append(field)
            
            if shipper_fields:
                logger.info(f"ğŸ” [LLM] å‘ç°å‘è´§æ–¹ç›¸å…³å­—æ®µ: {shipper_fields}")
            else:
                logger.warning(f"âš ï¸ [LLM] æœªå‘ç°å‘è´§æ–¹ç›¸å…³å­—æ®µï¼")
                logger.info(f"ğŸ” [LLM] æ‰€æœ‰å­—æ®µ: {field_names}")
        elif isinstance(excel_data, list):
            logger.info(f"ğŸ” [LLM] excel_dataæ˜¯åˆ—è¡¨ç±»å‹ï¼Œé•¿åº¦: {len(excel_data)}")
            if len(excel_data) > 0:
                logger.info(f"ğŸ” [LLM] ç¬¬ä¸€è¡Œæ•°æ®ç±»å‹: {type(excel_data[0])}")
                if isinstance(excel_data[0], dict):
                    field_names = list(excel_data[0].keys())
                    logger.info(f"ğŸ” [LLM] åˆ—è¡¨ä¸­çš„å­—æ®µ: {field_names}")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å‘è´§æ–¹ç›¸å…³çš„å­—æ®µ
                    shipper_keywords = [
                        "ç«™ç‚¹", "ç½‘ç‚¹", "é—¨åº—", "ä»“åº“", "é…é€ä¸­å¿ƒ", "DC", "å·¥å‚",
                        "ç«™å", "ç½‘å", "åº—å", "ä»“å", "å‚å",
                        "ç«™å·", "ç½‘å·", "åº—å·", "ä»“å·", "å‚å·",
                        "ç«™ç¼–ç ", "ç½‘ç¼–ç ", "åº—ç¼–ç ", "ä»“ç¼–ç ", "å‚ç¼–ç ",
                        "å‘è´§", "å‘é€", "å§‹å‘", "èµ·è¿", "å‘è¿"
                    ]
                    
                    shipper_fields = []
                    for field in field_names:
                        if any(keyword in field for keyword in shipper_keywords):
                            shipper_fields.append(field)
                    
                    if shipper_fields:
                        logger.info(f"ğŸ” [LLM] å‘ç°å‘è´§æ–¹ç›¸å…³å­—æ®µ: {shipper_fields}")
                    else:
                        logger.warning(f"âš ï¸ [LLM] æœªå‘ç°å‘è´§æ–¹ç›¸å…³å­—æ®µï¼")
                        logger.info(f"ğŸ” [LLM] æ‰€æœ‰å­—æ®µ: {field_names}")
        else:
            logger.warning(f"âš ï¸ [LLM] excel_dataæ˜¯æœªçŸ¥ç±»å‹: {type(excel_data)}")
        
        # è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
        excel_json_str = json.dumps(excel_data, ensure_ascii=False)
        logger.info(f"ğŸ“Š å‡†å¤‡å‘é€ç»™LLMçš„æ•°æ®é•¿åº¦: {len(excel_json_str)} å­—ç¬¦")
        
        # ä½¿ç”¨test_promptè¿›è¡ŒLLMåˆ†æ
        try:
            system_template = test_prompt.messages[0].prompt.template
            formatted_system_content = system_template.format(excel_data=excel_json_str)
            
            prompt = [{"role": "system", "content": formatted_system_content}]
            
            # è°ƒç”¨LLM
            response = chat_model.invoke(prompt)
            result_content = response.content
            
            # æ·»åŠ è°ƒè¯•æ—¥å¿— - æ˜¾ç¤ºLLMçš„åŸå§‹å“åº”
            logger.info(f"ğŸ” [LLM] LLMåŸå§‹å“åº”é•¿åº¦: {len(result_content)} å­—ç¬¦")
            logger.info(f"ğŸ” [LLM] LLMå“åº”å‰100å­—ç¬¦: {result_content[:100]}...")
            
            # å¢å¼ºçš„LLMå“åº”è§£æå™¨
            try:
                initial_mapping = parse_llm_response_robust(result_content)
                
                if initial_mapping:
                    # LLMè§£ææˆåŠŸï¼Œä½¿ç”¨LLMç»“æœ
                    state["initial_mapping"] = initial_mapping
                    state["confidence_score"] = 85.0  # é»˜è®¤ç½®ä¿¡åº¦
                    state["messages"].append({
                        "role": "assistant",
                        "content": f"LLMå­—æ®µæ˜ å°„å®Œæˆï¼Œæ˜ å°„äº† {len([v for v in initial_mapping.values() if v != 'missing'])} ä¸ªå­—æ®µï¼Œç½®ä¿¡åº¦: 85%"
                    })
                    
                    # ç›¸ä¼¼åº¦ç²¾æ’è¡¥å…¨ï¼ˆä»…è¡¥å…¨missingï¼Œä¸è¦†ç›–LLMå·²æœ‰æ˜ å°„ï¼‰
                    try:
                        pre_fields = state.get("preprocessed_fields", [])
                        sim_updated = apply_similarity_fill(state["initial_mapping"], pre_fields, threshold=0.4, only_missing=True)
                        if sim_updated != state["initial_mapping"]:
                            filled_cnt = len([1 for k in sim_updated if sim_updated[k] != "missing" and state["initial_mapping"].get(k, "missing") == "missing"]) 
                            logger.info(f"ğŸ” [ç›¸ä¼¼åº¦è¡¥å…¨] åœ¨LLMç»“æœåŸºç¡€ä¸Šè¡¥å…¨ {filled_cnt} ä¸ªå­—æ®µ")
                            state["initial_mapping"] = sim_updated
                    except Exception as _e:
                        logger.warning(f"âš ï¸ [ç›¸ä¼¼åº¦è¡¥å…¨] LLMåè¡¥å…¨å¤±è´¥: {_e}")
                else:
                    # LLMè§£æå¤±è´¥ï¼Œå°è¯•æ··åˆç­–ç•¥
                    logger.warning("âš ï¸ LLMå“åº”è§£æå¤±è´¥ï¼Œå°è¯•æ··åˆç­–ç•¥")
                    initial_mapping = try_hybrid_mapping(state, result_content)
                    state["initial_mapping"] = initial_mapping
                    state["messages"].append({
                        "role": "system",
                        "content": f"æ··åˆç­–ç•¥å®Œæˆï¼Œæ˜ å°„äº† {len([v for v in initial_mapping.values() if v != 'missing'])} ä¸ªå­—æ®µ"
                    })
                    
            except Exception as parse_error:
                logger.error(f"LLMå“åº”å¤„ç†å¤±è´¥: {str(parse_error)}")
                state["errors"].append(f"LLMå“åº”å¤„ç†å¤±è´¥: {str(parse_error)}")
                # æœ€åæ‰é™çº§åˆ°è§„åˆ™åŒ¹é…
                return rule_based_mapping_fallback(state)
                
        except Exception as llm_error:
            state["errors"].append(f"LLMè°ƒç”¨å¤±è´¥: {str(llm_error)}")
            # é™çº§åˆ°è§„åˆ™åŒ¹é…
            return rule_based_mapping_fallback(state)
        
        return state
        
    except Exception as e:
        state["errors"].append(f"LLMå­—æ®µåŒ¹é…å¤±è´¥: {str(e)}")
        # é™çº§åˆ°è§„åˆ™åŒ¹é…
        return rule_based_mapping_fallback(state)

# æ–°å¢ï¼šè§„åˆ™åŒ¹é…é™çº§æ–¹æ¡ˆ
def rule_based_mapping_fallback(state: FieldMappingState) -> FieldMappingState:
    """è§„åˆ™åŒ¹é…é™çº§æ–¹æ¡ˆ - å¢å¼ºç‰ˆ"""
    try:
        logger.info("ğŸ” [è§„åˆ™é™çº§] å¼€å§‹è§„åˆ™åŒ¹é…é™çº§")
        
        # ä½¿ç”¨å¢å¼ºçš„è§„åˆ™æ˜ å°„å‡½æ•°
        initial_mapping = get_rule_based_mapping(state)
        
        # æ™ºèƒ½è¡¥å……æ˜ å°„ï¼šä½¿ç”¨é…ç½®çš„æ˜ å°„è§„åˆ™è¿›è¡Œè¡¥å……
        try:
            from src.configs.manual_mapping_rules import get_mapping_rules
            
            mapping_rules = get_mapping_rules()
            preprocessed_fields = state.get("preprocessed_fields", [])
            
            # è·å–æ‰€æœ‰å­—æ®µåç§°
            available_fields = [field["name"] for field in preprocessed_fields]
            logger.info(f"ğŸ” [è§„åˆ™å›é€€] å¯ç”¨å­—æ®µ: {available_fields}")
            
            # åº”ç”¨æ˜ å°„è§„åˆ™
            for standard_field, keywords in mapping_rules.items():
                if initial_mapping.get(standard_field) == "missing":
                    for field_name in available_fields:
                        if any(keyword in field_name for keyword in keywords):
                            initial_mapping[standard_field] = field_name
                            logger.info(f"ğŸ” [è§„åˆ™å›é€€] æ™ºèƒ½è¡¥å…… {standard_field}: {field_name}")
                            break
                            
        except ImportError:
            logger.warning("âš ï¸ [è§„åˆ™å›é€€] æ— æ³•å¯¼å…¥æ‰‹åŠ¨æ˜ å°„è§„åˆ™ï¼Œä½¿ç”¨å†…ç½®è§„åˆ™")
            # ä½¿ç”¨å†…ç½®è§„åˆ™ä½œä¸ºå¤‡é€‰
            if initial_mapping.get("å‘è´§æ–¹ç¼–å·") == "missing" or initial_mapping.get("å‘è´§æ–¹åç§°") == "missing":
                logger.info("ğŸ” [è§„åˆ™å›é€€] å°è¯•æ™ºèƒ½è¡¥å……å‘è´§æ–¹æ˜ å°„...")
                
                # å‘è´§æ–¹ç›¸å…³å…³é”®è¯
                shipper_id_keywords = ["ç«™ç‚¹ç¼–å·", "ç½‘ç‚¹ç¼–å·", "é—¨åº—ç¼–å·", "ä»“åº“ç¼–å·", "é…é€ä¸­å¿ƒç¼–å·", "DCç¼–å·", "å·¥å‚ç¼–å·", "ç«™å·", "ç½‘å·", "åº—å·", "ä»“å·", "å‚å·"]
                shipper_name_keywords = ["ç«™ç‚¹åç§°", "ç½‘ç‚¹åç§°", "é—¨åº—åç§°", "ä»“åº“åç§°", "é…é€ä¸­å¿ƒåç§°", "DCåç§°", "å·¥å‚åç§°", "ç«™å", "ç½‘å", "åº—å", "ä»“å", "å‚å"]
                
                # å°è¯•åŒ¹é…å‘è´§æ–¹ç¼–å·
                if initial_mapping.get("å‘è´§æ–¹ç¼–å·") == "missing":
                    for field in preprocessed_fields:
                        field_name = field["name"]
                        if any(keyword in field_name for keyword in shipper_id_keywords):
                            initial_mapping["å‘è´§æ–¹ç¼–å·"] = field_name
                            logger.info(f"ğŸ” [è§„åˆ™å›é€€] æ™ºèƒ½è¡¥å……å‘è´§æ–¹ç¼–å·: {field_name}")
                            break
                
                # å°è¯•åŒ¹é…å‘è´§æ–¹åç§°
                if initial_mapping.get("å‘è´§æ–¹åç§°") == "missing":
                    for field in preprocessed_fields:
                        field_name = field["name"]
                        if any(keyword in field_name for keyword in shipper_name_keywords):
                            initial_mapping["å‘è´§æ–¹åç§°"] = field_name
                            logger.info(f"ğŸ” [è§„åˆ™å›é€€] æ™ºèƒ½è¡¥å……å‘è´§æ–¹åç§°: {field_name}")
                            break
        
        # ç›¸ä¼¼åº¦ç²¾æ’è¡¥å…¨ï¼ˆä»…è¡¥å…¨missingï¼‰
        try:
            pre_fields = state.get("preprocessed_fields", [])
            sim_updated = apply_similarity_fill(initial_mapping, pre_fields, threshold=0.4, only_missing=True)
            if sim_updated != initial_mapping:
                filled_cnt = len([1 for k in sim_updated if sim_updated[k] != "missing" and initial_mapping.get(k, "missing") == "missing"]) 
                logger.info(f"ğŸ” [ç›¸ä¼¼åº¦è¡¥å…¨] åœ¨è§„åˆ™é™çº§ç»“æœåŸºç¡€ä¸Šè¡¥å…¨ {filled_cnt} ä¸ªå­—æ®µ")
                initial_mapping = sim_updated
        except Exception as _e:
            logger.warning(f"âš ï¸ [ç›¸ä¼¼åº¦è¡¥å…¨] è§„åˆ™é™çº§åè¡¥å…¨å¤±è´¥: {_e}")
        
        # # ç¡®ä¿æ‰€æœ‰æ ‡å‡†å­—æ®µéƒ½æœ‰æ˜ å°„
        # for field in standard_fields:
        #     if field not in initial_mapping:
        #         initial_mapping[field] = "missing"
        
        state["initial_mapping"] = initial_mapping
        state["messages"].append({"role": "system", "content": f"è§„åˆ™åŒ¹é…é™çº§å®Œæˆï¼Œæ˜ å°„äº† {len([v for v in initial_mapping.values() if v != 'missing'])} ä¸ªå­—æ®µ"})
        
        return state
        
    except Exception as e:
        state["errors"].append(f"è§„åˆ™åŒ¹é…é™çº§å¤±è´¥: {str(e)}")
        return state

# æ–°å¢ï¼šæ˜ å°„è¯„åˆ†å‡½æ•°
def calculate_mapping_score(standard_field: str, field: Dict[str, Any]) -> float:
    """è®¡ç®—å­—æ®µæ˜ å°„çš„åŒ¹é…åˆ†æ•°"""
    score = 0.0
    field_name = field["name"].lower()
    
    # åŸºäºå­—æ®µåçš„åŒ¹é…
    if standard_field == "è®¢å•å·":
        if any(keyword in field_name for keyword in ["è®¢å•", "å•å·", "order", "è®¢å•å·"]):
            score += 0.6
        if field["uniqueness"] > 0.9:
            score += 0.4
    elif standard_field == "è¿å•å·":
        if any(keyword in field_name for keyword in ["è¿å•", "è¿å•å·", "waybill", "è¿å•å·"]):
            score += 0.6
        if field["uniqueness"] > 0.7:
            score += 0.3
    elif standard_field == "è¿è¾“æ—¥æœŸ":
        if any(keyword in field_name for keyword in ["è¿è¾“", "å‘è¿", "æ—¥æœŸ", "æ—¶é—´", "date", "time"]):
            score += 0.6
        if field["data_type"] == "date":
            score += 0.4
    elif standard_field == "ä»¶æ•°":
        if any(keyword in field_name for keyword in ["ä»¶æ•°", "æ•°é‡", "count", "qty", "ä»¶", "æ€»ä»¶æ•°", "ä»¶æ•°åˆè®¡", "ä»¶æ•°æ€»è®¡", "ç®±æ•°", "æ€»ç®±æ•°", "åŒ…æ•°", "æ€»åŒ…æ•°", "pcs", "box"]):
            score += 0.8
        if field["data_type"] == "numeric":
            score += 0.2
        # é¢å¤–åŠ åˆ†ï¼šå¦‚æœå­—æ®µååŒ…å«"æ€»"ã€"åˆè®¡"ã€"æ€»è®¡"ç­‰
        if any(keyword in field_name for keyword in ["æ€»", "åˆè®¡", "æ€»è®¡", "total", "sum"]):
            score += 0.3
    elif standard_field == "ä½“ç§¯":
        if any(keyword in field_name for keyword in ["ä½“ç§¯", "volume", "ç«‹æ–¹", "m3", "æ€»ä½“ç§¯", "ä½“ç§¯åˆè®¡", "ä½“ç§¯æ€»è®¡", "å®¹ç§¯", "ç©ºé—´", "ä½“ç§¯é‡", "vol", "cbm", "cubic"]):
            score += 0.8
        if field["data_type"] == "numeric":
            score += 0.2
        # é¢å¤–åŠ åˆ†ï¼šå¦‚æœå­—æ®µååŒ…å«"æ€»"ã€"åˆè®¡"ã€"æ€»è®¡"ç­‰
        if any(keyword in field_name for keyword in ["æ€»", "åˆè®¡", "æ€»è®¡", "total", "sum"]):
            score += 0.3
    elif standard_field == "é‡é‡":
        if any(keyword in field_name for keyword in ["é‡é‡", "weight", "å…¬æ–¤", "kg", "é‡", "æ€»é‡é‡", "é‡é‡åˆè®¡", "é‡é‡æ€»è®¡", "åƒå…‹", "å¨", "æ–¤", "ç£…", "wt", "ton", "lb"]):
            score += 0.8
        if field["data_type"] == "numeric":
            score += 0.2
        # é¢å¤–åŠ åˆ†ï¼šå¦‚æœå­—æ®µååŒ…å«"æ€»"ã€"åˆè®¡"ã€"æ€»è®¡"ç­‰
        if any(keyword in field_name for keyword in ["æ€»", "åˆè®¡", "æ€»è®¡", "total", "sum"]):
            score += 0.3
    elif standard_field == "æ‰¿è¿å•†":
        # æ‰©å±•æ‰¿è¿å•†çš„å…³é”®è¯è¦†ç›–
        if any(keyword in field_name for keyword in [
            "æ‰¿è¿å•†", "ç‰©æµ", "è¿è¾“", "carrier", "logistics",
            "è¿è¾“å…¬å¸", "ç‰©æµå…¬å¸", "å¿«é€’å…¬å¸", "é…é€å…¬å¸",  # æ–°å¢ï¼šå…¬å¸ç±»å‹
            "ä¾›åº”å•†", "åˆ†åŒ…å•†", "æŒ‡å®šæ‰¿è¿å•†", "åˆä½œæ‰¿è¿å•†"  # æ–°å¢ï¼šä¾›åº”å•†ç›¸å…³
        ]):
            score += 0.7
        if field["category"] == "name":
            score += 0.3
    elif standard_field == "è½¦å‹":
        if any(keyword in field_name for keyword in ["è½¦å‹", "è½¦è¾†", "vehicle", "truck", "è½¦"]):
            score += 0.8
        if field["category"] == "name":
            score += 0.2
    elif standard_field == "å‘è´§æ–¹åç§°":
        # æ‰©å±•å‘è´§æ–¹åç§°çš„å…³é”®è¯è¦†ç›–
        if any(keyword in field_name for keyword in [
            "å‘è´§", "å‘é€", "sender", "shipper", "å‘",
            "ç«™ç‚¹", "ç½‘ç‚¹", "é—¨åº—", "ä»“åº“", "é…é€ä¸­å¿ƒ", "DC", "å·¥å‚",  # æ–°å¢ï¼šç«™ç‚¹/ç½‘ç‚¹/é—¨åº—ç­‰
            "ç«™å", "ç½‘å", "åº—å", "ä»“å", "å‚å",  # æ–°å¢ï¼šç«™ç‚¹/ç½‘ç‚¹/é—¨åº—ç­‰çš„åç§°
            "å‘è´§ç‚¹", "å§‹å‘ç‚¹", "èµ·è¿ç‚¹", "å‘è¿ç‚¹"  # æ–°å¢ï¼šå‘è´§ç‚¹ç›¸å…³
        ]):
            score += 0.7
        if field["category"] == "name":
            score += 0.3
    elif standard_field == "æ”¶è´§æ–¹åç§°":
        if any(keyword in field_name for keyword in ["æ”¶è´§", "æ¥æ”¶", "receiver", "consignee", "æ”¶"]):
            score += 0.7
        if field["category"] == "name":
            score += 0.3
    elif standard_field == "å‘è´§æ–¹ç¼–å·":
        # æ‰©å±•å‘è´§æ–¹ç¼–å·çš„å…³é”®è¯è¦†ç›–
        if any(keyword in field_name for keyword in [
            "å‘è´§", "å‘é€", "sender", "shipper", "å‘"
        ]) and any(keyword in field_name for keyword in [
            "ç¼–å·", "ç¼–ç ", "code", "id"
        ]):
            score += 0.8
        # æ–°å¢ï¼šç«™ç‚¹/ç½‘ç‚¹/é—¨åº—/ä»“åº“/å·¥å‚ç¼–å·çš„è¯†åˆ«ï¼ˆä¸éœ€è¦åŒæ—¶åŒ…å«"å‘è´§"å…³é”®è¯ï¼‰
        elif any(keyword in field_name for keyword in [
            "ç«™ç‚¹ç¼–å·", "ç½‘ç‚¹ç¼–å·", "é—¨åº—ç¼–å·", "ä»“åº“ç¼–å·", "é…é€ä¸­å¿ƒç¼–å·", "DCç¼–å·", "å·¥å‚ç¼–å·",
            "ç«™å·", "ç½‘å·", "åº—å·", "ä»“å·", "å‚å·", "é…é€å·"
        ]):
            score += 0.9  # ç»™è¿™äº›æ˜ç¡®çš„å‘è´§æ–¹ç¼–å·æ›´é«˜åˆ†æ•°
        # æ–°å¢ï¼šç«™ç‚¹/ç½‘ç‚¹/é—¨åº—/ä»“åº“/å·¥å‚çš„ç¼–ç è¯†åˆ«
        elif any(keyword in field_name for keyword in [
            "ç«™ç‚¹ç¼–ç ", "ç½‘ç‚¹ç¼–ç ", "é—¨åº—ç¼–ç ", "ä»“åº“ç¼–ç ", "é…é€ä¸­å¿ƒç¼–ç ", "DCç¼–ç ", "å·¥å‚ç¼–ç ",
            "ç«™ç¼–ç ", "ç½‘ç¼–ç ", "åº—ç¼–ç ", "ä»“ç¼–ç ", "å‚ç¼–ç "
        ]):
            score += 0.9  # ç»™è¿™äº›æ˜ç¡®çš„å‘è´§æ–¹ç¼–ç æ›´é«˜åˆ†æ•°
        if field["category"] == "identifier":
            score += 0.2
    elif standard_field == "æ”¶è´§æ–¹ç¼–ç ":
        if any(keyword in field_name for keyword in ["æ”¶è´§", "æ¥æ”¶", "receiver"]) and any(keyword in field_name for keyword in ["ç¼–å·", "ç¼–ç ", "code", "id"]):
            score += 0.8
        # æ–°å¢ï¼šç›´æ¥è¯†åˆ«é€è´§ç‚¹ã€é…é€ç‚¹ç­‰æ”¶è´§ç›¸å…³å­—æ®µ
        elif any(keyword in field_name for keyword in ["é€è´§ç‚¹", "é€è´§ç‚¹ç¼–å·", "é…é€ç‚¹", "é…é€ç‚¹ç¼–å·", "ç›®çš„åœ°", "ç›®çš„åœ°ç¼–å·", "å®¢æˆ·", "å®¢æˆ·ç¼–å·", "å®¢æˆ·å·", "ç»ˆç«¯", "ç»ˆç«¯ç¼–å·"]):
            score += 0.9  # ç»™è¿™äº›æ˜ç¡®çš„æ”¶è´§æ–¹å­—æ®µæ›´é«˜åˆ†æ•°
        # é¢å¤–åŠ åˆ†ï¼šå¦‚æœå­—æ®µååŒ…å«æ”¶è´§ç›¸å…³è¯æ±‡
        if any(keyword in field_name.lower() for keyword in ["é€è´§", "æ”¶è´§", "é…é€", "ç›®çš„åœ°", "å®¢æˆ·", "ç»ˆç«¯"]):
            score += 0.3
        if field["category"] == "identifier":
            score += 0.2
    elif standard_field == "æ”¶è´§æ–¹åç§°":
        if any(keyword in field_name for keyword in ["æ”¶è´§", "æ¥æ”¶", "receiver"]) and any(keyword in field_name for keyword in ["åç§°", "name", "æ”¶è´§æ–¹åç§°"]):
            score += 0.8
        # æ–°å¢ï¼šç›´æ¥è¯†åˆ«é€è´§ç‚¹ã€é…é€ç‚¹ç­‰æ”¶è´§ç›¸å…³å­—æ®µ
        elif any(keyword in field_name for keyword in ["é€è´§ç‚¹", "é€è´§ç‚¹åç§°", "é…é€ç‚¹", "é…é€ç‚¹åç§°", "ç›®çš„åœ°", "ç›®çš„åœ°åç§°", "å®¢æˆ·", "å®¢æˆ·åç§°", "å®¢æˆ·å", "ç»ˆç«¯", "ç»ˆç«¯åç§°"]):
            score += 0.9  # ç»™è¿™äº›æ˜ç¡®çš„æ”¶è´§æ–¹å­—æ®µæ›´é«˜åˆ†æ•°
        # é¢å¤–åŠ åˆ†ï¼šå¦‚æœå­—æ®µååŒ…å«æ”¶è´§ç›¸å…³è¯æ±‡
        if any(keyword in field_name.lower() for keyword in ["é€è´§", "æ”¶è´§", "é…é€", "ç›®çš„åœ°", "å®¢æˆ·", "ç»ˆç«¯"]):
            score += 0.3
        if field["category"] == "name":
            score += 0.2
    elif standard_field == "å•†å“ç¼–ç ":
        # æ‰©å±•å•†å“ç¼–ç çš„å…³é”®è¯è¦†ç›–
        if any(keyword in field_name for keyword in [
            "å•†å“", "äº§å“", "product", "goods", "è´§ç‰©", "è´§å“"
        ]) and any(keyword in field_name for keyword in [
            "ç¼–å·", "ç¼–ç ", "code", "id", "è´§å·", "å“å·"
        ]):
            score += 0.8
        # æ–°å¢ï¼šç›´æ¥çš„å•†å“ç¼–ç è¯†åˆ«ï¼ˆä¸éœ€è¦åŒæ—¶åŒ…å«"å•†å“"å…³é”®è¯ï¼‰
        elif any(keyword in field_name for keyword in [
            "è´§å·", "å“å·", "SKU", "sku", "å•†å“ä»£ç ", "äº§å“ä»£ç ", "item id", "item id", "item code", "product code", "product id", "goods code", "material code"
        ]):
            score += 0.9  # ç»™è¿™äº›æ˜ç¡®çš„å•†å“ç¼–ç æ›´é«˜åˆ†æ•°
        # é¢å¤–åŠ åˆ†ï¼šå¦‚æœå­—æ®µååŒ…å«è‹±æ–‡å•†å“ç›¸å…³è¯æ±‡
        if any(keyword in field_name.lower() for keyword in ["item", "product", "goods", "material", "sku"]):
            score += 0.3
        if field["category"] == "identifier":
            score += 0.2
    elif standard_field == "å•†å“åç§°":
        # æ‰©å±•å•†å“åç§°çš„å…³é”®è¯è¦†ç›–
        if any(keyword in field_name for keyword in [
            "å•†å“", "äº§å“", "product", "goods", "è´§ç‰©", "è´§å“"
        ]) and any(keyword in field_name for keyword in [
            "åç§°", "name", "å“å", "è´§å"
        ]):
            score += 0.8
        # æ–°å¢ï¼šç›´æ¥çš„å•†å“åç§°è¯†åˆ«ï¼ˆä¸éœ€è¦åŒæ—¶åŒ…å«"å•†å“"å…³é”®è¯ï¼‰
        elif any(keyword in field_name for keyword in [
            "å“å", "è´§å", "å•†å“æè¿°", "äº§å“æè¿°", "è´§ç‰©æè¿°", "item description", "item desc", "product name", "product description", "goods name", "material description"
        ]):
            score += 0.9  # ç»™è¿™äº›æ˜ç¡®çš„å•†å“åç§°æ›´é«˜åˆ†æ•°
        # é¢å¤–åŠ åˆ†ï¼šå¦‚æœå­—æ®µååŒ…å«è‹±æ–‡å•†å“ç›¸å…³è¯æ±‡
        if any(keyword in field_name.lower() for keyword in ["item", "product", "goods", "material", "description", "desc", "name"]):
            score += 0.3
        if field["category"] == "name":
            score += 0.2
    elif standard_field == "è·¯é¡º":
        if any(keyword in field_name for keyword in ["è·¯é¡º", "é¡ºåº", "sequence", "order", "è·¯çº¿"]):
            score += 0.9
        if field["data_type"] == "numeric":
            score += 0.1
    
    return score

# æ–°å¢ï¼šå†²çªä¸ç¼ºå¤±å¤„ç†èŠ‚ç‚¹
def resolve_conflicts_and_missing(state: FieldMappingState) -> FieldMappingState:
    """å†²çªä¸ç¼ºå¤±å¤„ç†ï¼šæ£€æŸ¥å†²çªå¹¶ä¿®æ­£æ˜ å°„"""
    try:
        initial_mapping = state.get("initial_mapping", {})
        
        # æ·»åŠ è°ƒè¯•æ—¥å¿— - å†²çªå¤„ç†å‰çš„æ˜ å°„çŠ¶æ€
        logger.info(f"ğŸ” [å†²çªå¤„ç†] å¼€å§‹å¤„ç†å†²çªä¸ç¼ºå¤±")
        logger.info(f"ğŸ” [å†²çªå¤„ç†] å†²çªå¤„ç†å‰çš„æ˜ å°„: {json.dumps(initial_mapping, ensure_ascii=False)}")
        
        # å¦‚æœinitial_mappingä¸ºç©ºï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤æ˜ å°„
        if not initial_mapping:
            logger.warning("âš ï¸ initial_mappingä¸ºç©ºï¼Œåˆ›å»ºé»˜è®¤æ˜ å°„")
            standard_fields = [
                "è¿è¾“æ—¥æœŸ", "è®¢å•å·", "è·¯é¡º", "æ‰¿è¿å•†", "è¿å•å·", "è½¦å‹", 
                "å‘è´§æ–¹ç¼–å·", "å‘è´§æ–¹åç§°", "æ”¶è´§æ–¹ç¼–ç ", "æ”¶è´§æ–¹åç§°", 
                "å•†å“ç¼–ç ", "å•†å“åç§°", "ä»¶æ•°", "ä½“ç§¯", "é‡é‡"
            ]
            initial_mapping = {field: "missing" for field in standard_fields}
        
        final_mapping = initial_mapping.copy()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªä¸ªæ€§åŒ–å­—æ®µæ˜ å°„åˆ°åŒä¸€ä¸ªæ ‡å‡†åŒ–å­—æ®µ
        value_counts = {}
        for standard_field, personalized_field in initial_mapping.items():
            if personalized_field != "missing":
                if personalized_field in value_counts:
                    value_counts[personalized_field].append(standard_field)
                else:
                    value_counts[personalized_field] = [standard_field]
        
        # æ·»åŠ è°ƒè¯•æ—¥å¿— - æ£€æµ‹åˆ°çš„å†²çª
        if value_counts:
            logger.info(f"ğŸ” [å†²çªå¤„ç†] æ£€æµ‹åˆ°çš„å­—æ®µæ˜ å°„å…³ç³»:")
            for personalized_field, standard_fields in value_counts.items():
                if len(standard_fields) > 1:
                    logger.info(f"ğŸ” [å†²çªå¤„ç†] å†²çª: {personalized_field} -> {standard_fields}")
                else:
                    logger.info(f"ğŸ” [å†²çªå¤„ç†] æ­£å¸¸: {personalized_field} -> {standard_fields}")
        else:
            logger.info(f"ğŸ” [å†²çªå¤„ç†] æ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•å­—æ®µæ˜ å°„å…³ç³»")
        
        # å¤„ç†å†²çª
        conflict_count = 0
        for personalized_field, standard_fields in value_counts.items():
            if len(standard_fields) > 1:
                conflict_count += 1
                logger.info(f"ğŸ” [å†²çªå¤„ç†] å¤„ç†ç¬¬{conflict_count}ä¸ªå†²çª: {personalized_field} -> {standard_fields}")
                
                # ä¿ç•™ä¼˜å…ˆçº§æœ€é«˜çš„æ˜ å°„ï¼Œå…¶ä»–è®¾ä¸ºmissing
                priority_order = ["ä»¶æ•°", "ä½“ç§¯", "é‡é‡", "å•†å“ç¼–ç ", "å•†å“åç§°", "æ”¶è´§æ–¹ç¼–ç ", "æ”¶è´§æ–¹åç§°", "å‘è´§æ–¹ç¼–å·", "å‘è´§æ–¹åç§°", "è¿è¾“æ—¥æœŸ", "è®¢å•å·","è¿å•å·"]
                
                # è®¡ç®—æ¯ä¸ªå­—æ®µçš„ä¼˜å…ˆçº§åˆ†æ•°
                field_scores = {}
                for field in standard_fields:
                    if field in priority_order:
                        score = priority_order.index(field)
                        field_scores[field] = score
                        logger.info(f"ğŸ” [å†²çªå¤„ç†] {field} ä¼˜å…ˆçº§åˆ†æ•°: {score}")
                    else:
                        field_scores[field] = 999
                        logger.info(f"ğŸ” [å†²çªå¤„ç†] {field} ä¼˜å…ˆçº§åˆ†æ•°: 999 (ä¸åœ¨ä¼˜å…ˆçº§åˆ—è¡¨ä¸­)")
                
                best_field = max(standard_fields, key=lambda x: priority_order.index(x) if x in priority_order else 999)
                logger.info(f"ğŸ” [å†²çªå¤„ç†] é€‰æ‹©ä¿ç•™: {best_field} (ä¼˜å…ˆçº§åˆ†æ•°: {field_scores[best_field]})")
                
                for field in standard_fields:
                    if field != best_field:
                        final_mapping[field] = "missing"
                        logger.info(f"ğŸ” [å†²çªå¤„ç†] è®¾ä¸ºmissing: {field} (ä¼˜å…ˆçº§åˆ†æ•°: {field_scores[field]})")
        
        if conflict_count == 0:
            logger.info(f"ğŸ” [å†²çªå¤„ç†] æ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•å†²çª")
        
        # æ·»åŠ è°ƒè¯•æ—¥å¿— - å†²çªå¤„ç†åçš„æ˜ å°„çŠ¶æ€
        logger.info(f"ğŸ” [å†²çªå¤„ç†] å†²çªå¤„ç†åçš„æ˜ å°„: {json.dumps(final_mapping, ensure_ascii=False)}")
        
        # æ£€æŸ¥å‘è´§æ–¹ç›¸å…³å­—æ®µçš„çŠ¶æ€å˜åŒ–
        shipper_fields = ["å‘è´§æ–¹åç§°", "å‘è´§æ–¹ç¼–å·"]
        for field in shipper_fields:
            if initial_mapping.get(field) != final_mapping.get(field):
                logger.warning(f"âš ï¸ [å†²çªå¤„ç†] {field} çŠ¶æ€å‘ç”Ÿå˜åŒ–: {initial_mapping.get(field)} -> {final_mapping.get(field)}")
        
        state["final_mapping"] = final_mapping
        state["messages"].append({"role": "system", "content": "å†²çªä¸ç¼ºå¤±å¤„ç†å®Œæˆ"})
        return state
        
    except Exception as e:
        state["errors"].append(f"å†²çªä¸ç¼ºå¤±å¤„ç†å¤±è´¥: {str(e)}")
        return state

# æ–°å¢ï¼šæ˜ å°„éªŒè¯èŠ‚ç‚¹
def validate_mapping(state: FieldMappingState) -> FieldMappingState:
    """æ˜ å°„éªŒè¯ï¼šä¸šåŠ¡é€»è¾‘æ ¡éªŒ"""
    try:
        final_mapping = state["final_mapping"]
        validation_results = {"passed": True, "issues": []}
        
        # æ”¾å®½éªŒè¯æ¡ä»¶ - ä¸å†å¼ºåˆ¶è¦æ±‚ä»»ä½•å­—æ®µ
        important_fields = ["è®¢å•å·", "è¿å•å·", "è¿è¾“æ—¥æœŸ"]  # é‡è¦å­—æ®µï¼Œä»…æé†’
        
        # æ£€æŸ¥é‡è¦å­—æ®µï¼ˆä»…æé†’ï¼Œä¸å½±å“éªŒè¯é€šè¿‡ï¼‰
        missing_important = [field for field in important_fields if final_mapping.get(field) == "missing"]
        if missing_important:
            validation_results["issues"].append(f"å»ºè®®è¡¥å……é‡è¦å­—æ®µ: {', '.join(missing_important)}")
        
        # é€»è¾‘å…³ç³»éªŒè¯
        if final_mapping.get("è·¯é¡º") != "missing" and final_mapping.get("è¿å•å·") == "missing":
            validation_results["issues"].append("è·¯é¡ºå­—æ®µå­˜åœ¨ä½†ç¼ºå°‘è¿å•å·")
        
        # æ•°é‡å­—æ®µéªŒè¯
        quantity_fields = ["ä»¶æ•°", "ä½“ç§¯", "é‡é‡"]
        if all(final_mapping.get(field) == "missing" for field in quantity_fields):
            validation_results["issues"].append("ç¼ºå°‘æ‰€æœ‰æ•°é‡ç›¸å…³å­—æ®µ")
        
        # è®¾ç½®éªŒè¯é€šè¿‡ - ä¸å†å¼ºåˆ¶è¦æ±‚å…³é”®å­—æ®µ
        validation_results["passed"] = True
        
        state["validation_results"] = validation_results
        state["messages"].append({"role": "system", "content": f"æ˜ å°„éªŒè¯å®Œæˆï¼Œé€šè¿‡: {validation_results['passed']}"})
        return state
        
    except Exception as e:
        state["errors"].append(f"æ˜ å°„éªŒè¯å¤±è´¥: {str(e)}")
        return state

# æ–°å¢ï¼šå‡†ç¡®ç‡è¯„åˆ†èŠ‚ç‚¹
def calculate_confidence_score(state: FieldMappingState) -> FieldMappingState:
    """è®¡ç®—å‡†ç¡®ç‡è¯„åˆ†"""
    try:
        final_mapping = state["final_mapping"]
        validation_results = state["validation_results"]
        
        # å­—æ®µè¦†ç›–ç‡
        total_fields = len(final_mapping)
        mapped_fields = len([v for v in final_mapping.values() if v != "missing"])
        coverage_rate = mapped_fields / total_fields if total_fields > 0 else 0
        
        # å­—æ®µé‡è¦æ€§åˆ†å±‚è¯„åˆ† - è°ƒæ•´æƒé‡åˆ†é…
        important_fields = ["è®¢å•å·", "è¿å•å·", "è¿è¾“æ—¥æœŸ", "ä»¶æ•°", "ä½“ç§¯", "é‡é‡"]  # é‡è¦å­—æ®µ
        other_fields = ["è·¯é¡º", "æ‰¿è¿å•†", "è½¦å‹", "å‘è´§æ–¹åç§°", "æ”¶è´§æ–¹åç§°", "å•†å“åç§°"]  # ä¸€èˆ¬é‡è¦
        
        important_score = sum(1 for field in important_fields if final_mapping.get(field) != "missing") / len(important_fields)
        other_score = sum(1 for field in other_fields if final_mapping.get(field) != "missing") / len(other_fields)
        
        # éªŒè¯é€šè¿‡ç‡ - ç°åœ¨æ€»æ˜¯1.0ï¼Œå› ä¸ºä¸å†å¼ºåˆ¶è¦æ±‚å…³é”®å­—æ®µ
        validation_score = 1.0
        
        # ç»¼åˆè¯„åˆ† - é‡æ–°åˆ†é…æƒé‡
        confidence_score = (0.6 * important_score +    # é‡è¦å­—æ®µæƒé‡60%
                          0.3 * other_score +         # å…¶ä»–å­—æ®µæƒé‡30% 
                          0.1 * validation_score)     # éªŒè¯é€šè¿‡æƒé‡10%
        confidence_score = round(confidence_score * 100, 1)
        
        state["confidence_score"] = confidence_score
        state["messages"].append({"role": "system", "content": f"å‡†ç¡®ç‡è¯„åˆ†å®Œæˆ: {confidence_score}%"})
        return state
        
    except Exception as e:
        state["errors"].append(f"å‡†ç¡®ç‡è¯„åˆ†å¤±è´¥: {str(e)}")
        return state

# æ–°å¢ï¼šç»“æœç”ŸæˆèŠ‚ç‚¹
def generate_output(state: FieldMappingState) -> FieldMappingState:
    """ç”Ÿæˆæœ€ç»ˆè¾“å‡ºç»“æœ"""
    try:
        final_mapping = state.get("final_mapping", {})
        validation_results = state.get("validation_results", {"passed": True, "issues": []})
        confidence_score = state.get("confidence_score", 0.0)
        
        # å¦‚æœfinal_mappingä¸ºç©ºï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤æ˜ å°„
        if not final_mapping:
            logger.warning("âš ï¸ final_mappingä¸ºç©ºï¼Œåˆ›å»ºé»˜è®¤æ˜ å°„")
            standard_fields = [
                "è¿è¾“æ—¥æœŸ", "è®¢å•å·", "è·¯é¡º", "æ‰¿è¿å•†", "è¿å•å·", "è½¦å‹", 
                "å‘è´§æ–¹ç¼–å·", "å‘è´§æ–¹åç§°", "æ”¶è´§æ–¹ç¼–ç ", "æ”¶è´§æ–¹åç§°", 
                "å•†å“ç¼–ç ", "å•†å“åç§°", "ä»¶æ•°", "ä½“ç§¯", "é‡é‡"
            ]
            final_mapping = {field: "missing" for field in standard_fields}
        
        # ç”Ÿæˆåˆ†æä¾æ®
        analysis = {
            "ä¾æ®": f"åŸºäºå­—æ®µåç§°ç‰¹å¾å’Œæ•°æ®ç±»å‹åˆ†æï¼Œå…±æ˜ å°„äº† {len([v for v in final_mapping.values() if v != 'missing'])} ä¸ªå­—æ®µ",
            "æé†’": validation_results.get("issues", []),
            "å‡†ç¡®ç‡": f"{confidence_score}%"
        }
        
        # ç»„è£…æœ€ç»ˆç»“æœ
        output = {
            "mapping": final_mapping,
            "analysis": analysis,
            "confidence": int(confidence_score)
        }
        
        state["messages"].append({"role": "assistant", "content": json.dumps(output, ensure_ascii=False)})
        return state
        
    except Exception as e:
        state["errors"].append(f"ç»“æœç”Ÿæˆå¤±è´¥: {str(e)}")
        return state

# æ–°å¢ï¼šè¿­ä»£è®¡æ•°æ›´æ–°èŠ‚ç‚¹
def update_iteration_count(state: FieldMappingState) -> FieldMappingState:
    """æ›´æ–°è¿­ä»£è®¡æ•°"""
    state["iteration_count"] = state.get("iteration_count", 0) + 1
    state["messages"].append({"role": "system", "content": f"å¼€å§‹ç¬¬ {state['iteration_count']} æ¬¡è¿­ä»£"})
    return state

# æ–°å¢ï¼šæ¡ä»¶è·¯ç”±å‡½æ•°
def should_retry_mapping(state: FieldMappingState) -> str:
    """åˆ¤æ–­æ˜¯å¦éœ€è¦é‡æ–°æ˜ å°„"""
    # æ£€æŸ¥éªŒè¯æ˜¯å¦é€šè¿‡
    validation_passed = state.get("validation_results", {}).get("passed", True)
    iteration_count = state.get("iteration_count", 0)
    
    if validation_passed:
        return "generate_output"
    elif iteration_count < 3:  # æœ€å¤šè¿­ä»£3æ¬¡
        return "update_iteration"
    else:
        return "generate_output"

# æ–°å¢ï¼šåˆ›å»ºå­—æ®µæ˜ å°„å›¾
def create_field_mapping_graph() -> StateGraph:
    """åˆ›å»ºå­—æ®µæ˜ å°„çš„çŠ¶æ€å›¾"""
    try:
        graph_builder = StateGraph(FieldMappingState)
        
        # æ·»åŠ èŠ‚ç‚¹
        graph_builder.add_node("preprocess_fields", preprocess_fields)
        graph_builder.add_node("classify_fields", classify_fields)
        graph_builder.add_node("llm_mapping", llm_map_to_standard_fields)
        graph_builder.add_node("resolve_conflicts", resolve_conflicts_and_missing)
        graph_builder.add_node("validate_mapping", validate_mapping)
        graph_builder.add_node("calculate_confidence", calculate_confidence_score)
        graph_builder.add_node("update_iteration", update_iteration_count)
        graph_builder.add_node("generate_output", generate_output)
        
        # æ·»åŠ è¾¹
        graph_builder.add_edge(START, "preprocess_fields")
        graph_builder.add_edge("preprocess_fields", "classify_fields")
        graph_builder.add_edge("classify_fields", "llm_mapping")
        graph_builder.add_edge("llm_mapping", "resolve_conflicts")
        graph_builder.add_edge("resolve_conflicts", "validate_mapping")
        graph_builder.add_edge("validate_mapping", "calculate_confidence")
        
        # æ¡ä»¶è·¯ç”±ï¼šä»calculate_confidenceå†³å®šä¸‹ä¸€æ­¥
        graph_builder.add_conditional_edges(
            "calculate_confidence",
            should_retry_mapping,
            {
                "update_iteration": "update_iteration",
                "generate_output": "generate_output"
            }
        )
        
        # è¿­ä»£è·¯å¾„ï¼šæ›´æ–°è®¡æ•°åé‡æ–°æ˜ å°„
        graph_builder.add_edge("update_iteration", "llm_mapping")
        
        # ç»“æŸ
        graph_builder.add_edge("generate_output", END)
        
        # è®¾ç½®é€’å½’é™åˆ¶
        return graph_builder.compile(checkpointer=None)
        
    except Exception as e:
        raise RuntimeError(f"Failed to create field mapping graph: {str(e)}")

# åˆ›å»ºå’Œé…ç½®chatbotçš„çŠ¶æ€å›¾
def create_graph(llm) -> StateGraph:
    try:
        graph_builder = StateGraph(State)
        def chatbot(state: State) -> dict:
            # å¤„ç†å½“å‰çŠ¶æ€å¹¶è¿”å› LLM å“åº”
            return {"messages": [llm.invoke(state["messages"])]}
        graph_builder.add_node("chatbot", chatbot)
        graph_builder.add_edge(START, "chatbot")
        graph_builder.add_edge("chatbot", END)
        # è¿™é‡Œä½¿ç”¨å†…å­˜å­˜å‚¨ ä¹Ÿå¯ä»¥æŒä¹…åŒ–åˆ°æ•°æ®åº“
        memory = MemorySaver()
        # return graph_builder.compile(checkpointer=memory)
        return graph_builder.compile()
    except Exception as e:
        raise RuntimeError(f"Failed to create graph: {str(e)}")

def save_graph_visualization(graph: StateGraph, filename: str = "graph.png") -> None:
    try:
        with open(filename, "wb") as f:
            f.write(graph.get_graph().draw_mermaid_png())
        logger.info(f"Graph visualization saved as {filename}")
    except IOError as e:
        logger.info(f"Warning: Failed to save graph visualization: {str(e)}")


# æ ¼å¼åŒ–å“åº”ï¼Œå¯¹è¾“å…¥çš„æ–‡æœ¬è¿›è¡Œæ®µè½åˆ†éš”ã€æ·»åŠ é€‚å½“çš„æ¢è¡Œç¬¦ï¼Œä»¥åŠåœ¨ä»£ç å—ä¸­å¢åŠ æ ‡è®°ï¼Œä»¥ä¾¿ç”Ÿæˆæ›´å…·å¯è¯»æ€§çš„è¾“å‡º
def format_response(response):
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ \n{2, }å°†è¾“å…¥çš„responseæŒ‰ç…§ä¸¤ä¸ªæˆ–æ›´å¤šçš„è¿ç»­æ¢è¡Œç¬¦è¿›è¡Œåˆ†å‰²ã€‚è¿™æ ·å¯ä»¥å°†æ–‡æœ¬åˆ†å‰²æˆå¤šä¸ªæ®µè½ï¼Œæ¯ä¸ªæ®µè½ç”±è¿ç»­çš„éç©ºè¡Œç»„æˆ
    paragraphs = re.split(r'\n{2,}', response)
    # ç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨æ ¼å¼åŒ–åçš„æ®µè½
    formatted_paragraphs = []
    # éå†æ¯ä¸ªæ®µè½è¿›è¡Œå¤„ç†
    for para in paragraphs:
        # æ£€æŸ¥æ®µè½ä¸­æ˜¯å¦åŒ…å«ä»£ç å—æ ‡è®°
        if '```' in para:
            # å°†æ®µè½æŒ‰ç…§```åˆ†å‰²æˆå¤šä¸ªéƒ¨åˆ†ï¼Œä»£ç å—å’Œæ™®é€šæ–‡æœ¬äº¤æ›¿å‡ºç°
            parts = para.split('```')
            for i, part in enumerate(parts):
                # æ£€æŸ¥å½“å‰éƒ¨åˆ†çš„ç´¢å¼•æ˜¯å¦ä¸ºå¥‡æ•°ï¼Œå¥‡æ•°éƒ¨åˆ†ä»£è¡¨ä»£ç å—
                if i % 2 == 1:  # è¿™æ˜¯ä»£ç å—
                    # å°†ä»£ç å—éƒ¨åˆ†ç”¨æ¢è¡Œç¬¦å’Œ```åŒ…å›´ï¼Œå¹¶å»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
                    parts[i] = f"\n```\n{part.strip()}\n```\n"
            # å°†åˆ†å‰²åçš„éƒ¨åˆ†é‡æ–°ç»„åˆæˆä¸€ä¸ªå­—ç¬¦ä¸²
            para = ''.join(parts)
        else:
            # å¦åˆ™ï¼Œå°†å¥å­ä¸­çš„å¥ç‚¹åé¢çš„ç©ºæ ¼æ›¿æ¢ä¸ºæ¢è¡Œç¬¦ï¼Œä»¥ä¾¿å¥å­ä¹‹é—´æœ‰æ˜ç¡®çš„åˆ†éš”
            para = para.replace('. ', '.\n')
        # å°†æ ¼å¼åŒ–åçš„æ®µè½æ·»åŠ åˆ°formatted_paragraphsåˆ—è¡¨
        # strip()æ–¹æ³•ç”¨äºç§»é™¤å­—ç¬¦ä¸²å¼€å¤´å’Œç»“å°¾çš„ç©ºç™½å­—ç¬¦ï¼ˆåŒ…æ‹¬ç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ \tã€æ¢è¡Œç¬¦ \nç­‰ï¼‰
        formatted_paragraphs.append(para.strip())
    # å°†æ‰€æœ‰æ ¼å¼åŒ–åçš„æ®µè½ç”¨ä¸¤ä¸ªæ¢è¡Œç¬¦è¿æ¥èµ·æ¥ï¼Œä»¥å½¢æˆä¸€ä¸ªå…·æœ‰æ¸…æ™°æ®µè½åˆ†éš”çš„æ–‡æœ¬
    return '\n\n'.join(formatted_paragraphs)


# # åˆ›å»º Nacos å®¢æˆ·ç«¯
# nacos_client = nacos.NacosClient(NACOS_SERVER, namespace=NAMESPACE)
# # åœæ­¢å¿ƒè·³çš„æ ‡å¿—
# stop_event = threading.Event()
# def heartbeat_loop():
#     while not stop_event.is_set():
#         try:
#             nacos_client.send_heartbeat(SERVICE_NAME, HOST, PORT, group_name=GROUP_NAME)
#             logger.info("ğŸ’“ å¿ƒè·³å‘é€æˆåŠŸ")
#         except Exception as e:
#             logger.info(f"âš ï¸ å¿ƒè·³å‘é€å¤±è´¥: {e}")
#         stop_event.wait(HEARTBEAT_INTERVAL)

from src.configs.nacos_helper import NacosHelper
from src.configs.settings import manager, RegisterConfig

@asynccontextmanager
async def lifespan(app: FastAPI):
    # å¯åŠ¨æ—¶æ‰§è¡Œ
    # ç”³æ˜å¼•ç”¨å…¨å±€å˜é‡ï¼Œåœ¨å‡½æ•°ä¸­è¢«åˆå§‹åŒ–ï¼Œå¹¶åœ¨æ•´ä¸ªåº”ç”¨ä¸­ä½¿ç”¨
    global graph, field_mapping_graph
    try:
        # æ³¨å†ŒæœåŠ¡
        # nacos_client = nacos.NacosClient(NACOS_SERVER, namespace=NAMESPACE)
        # nacos_client.add_naming_instance(SERVICE_NAME, HOST, PORT, group_name=GROUP_NAME)
        # logger.info(f"âœ… å·²æ³¨å†Œåˆ° Nacos: {SERVICE_NAME} @ {HOST}:{PORT}")
        # register_select = manager.get_register_config("nacos")
        # nacos_client = NacosHelper(register_select)
        # heartbeat_thread = threading.Thread(target=nacos_client.heartbeat_loop, daemon=True)
        # nacos_client.register_instance()
        # å¯åŠ¨å¿ƒè·³çº¿ç¨‹
        # heartbeat_thread.start()
        logger.info("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹ã€å®šä¹‰Graph...")
        graph = create_graph(chat_model)
        # æ–°å¢ï¼šåˆå§‹åŒ–å­—æ®µæ˜ å°„å›¾
        field_mapping_graph = create_field_mapping_graph()
        # save_graph_visualization(graph)
        logger.info("åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        # raise å…³é”®å­—é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œä»¥ç¡®ä¿ç¨‹åºä¸ä¼šåœ¨é”™è¯¯çŠ¶æ€ä¸‹ç»§ç»­è¿è¡Œ
        raise

    # yield å…³é”®å­—å°†æ§åˆ¶æƒäº¤è¿˜ç»™FastAPIæ¡†æ¶ï¼Œä½¿åº”ç”¨å¼€å§‹è¿è¡Œ
    # åˆ†éš”äº†å¯åŠ¨å’Œå…³é—­çš„é€»è¾‘ã€‚åœ¨yield ä¹‹å‰çš„ä»£ç åœ¨åº”ç”¨å¯åŠ¨æ—¶è¿è¡Œï¼Œyield ä¹‹åçš„ä»£ç åœ¨åº”ç”¨å…³é—­æ—¶è¿è¡Œ
    try:
        yield
    finally:
        print("ğŸ›‘ åœæ­¢æœåŠ¡ï¼Œæ³¨é”€å¹¶åœæ­¢å¿ƒè·³...")
        # nacos_client.stop_event.set()
        # heartbeat_thread.join()
        # nacos_client.deregister()
    # å…³é—­æ—¶æ‰§è¡Œ
    logger.info("æ­£åœ¨å…³é—­...")

# lifespanå‚æ•°ç”¨äºåœ¨åº”ç”¨ç¨‹åºç”Ÿå‘½å‘¨æœŸçš„å¼€å§‹å’Œç»“æŸæ—¶æ‰§è¡Œä¸€äº›åˆå§‹åŒ–æˆ–æ¸…ç†å·¥ä½œ
app = FastAPI(lifespan=lifespan)
# æ£€æŸ¥nacosæœåŠ¡æ˜¯å¦æ­£å¸¸ | curl "http://127.0.0.1:8848/nacos/v1/ns/instance/list?serviceName=fastapi-service-greet-system"
@app.get("/")
async def root():
    return {"message": "Hello from FastAPI with Nacos!"}

# æ–°å¢ï¼šå­—æ®µæ˜ å°„APIç«¯ç‚¹
@app.post("/field-mapping")
async def field_mapping(request: ChatCompletionRequest):
    """å­—æ®µæ˜ å°„APIï¼šå°†Excelå­—æ®µæ˜ å°„åˆ°æ ‡å‡†å­—æ®µ"""
    if not field_mapping_graph:
        logger.error("å­—æ®µæ˜ å°„æœåŠ¡æœªåˆå§‹åŒ–")
        raise HTTPException(status_code=500, detail="å­—æ®µæ˜ å°„æœåŠ¡æœªåˆå§‹åŒ–")

    try:
        logger.info(f"æ”¶åˆ°å­—æ®µæ˜ å°„è¯·æ±‚: {request}")

        # è·å–Excelæ•°æ®
        excel_data = None
        if request.excelData:
            try:
                excel_data = json.loads(request.excelData)
                logger.info(f"ä½¿ç”¨æä¾›çš„Excelæ•°æ®")
            except json.JSONDecodeError:
                raise HTTPException(status_code=400, detail="Excelæ•°æ®æ ¼å¼é”™è¯¯")
        elif request.excelFilename:
            try:
                excel_result = excel_processor.read_excel_to_json(request.excelFilename)
                excel_data = excel_result["data"]  # ç›´æ¥ä½¿ç”¨è¡Œè®°å½•åˆ—è¡¨
                logger.info(f"æˆåŠŸè¯»å–Excelæ–‡ä»¶ï¼Œå…±{excel_result['total_rows']}è¡Œæ•°æ®")
            except FileNotFoundError:
                raise HTTPException(status_code=404, detail=f"Excelæ–‡ä»¶ä¸å­˜åœ¨: {request.excelFilename}")
            except Exception as e:
                logger.error(f"è¯»å–Excelæ–‡ä»¶å¤±è´¥: {str(e)}")
                raise HTTPException(status_code=500, detail=f"è¯»å–Excelæ–‡ä»¶å¤±è´¥: {str(e)}")
        else:
            raise HTTPException(status_code=400, detail="ç¼ºå°‘Excelæ•°æ®æˆ–æ–‡ä»¶å")

        # åˆå§‹åŒ–çŠ¶æ€
        initial_state = FieldMappingState(
            excel_data=excel_data,
            preprocessed_fields=[],
            classified_fields=[],
            initial_mapping={},
            final_mapping={},
            validation_results={"passed": True, "issues": []},  # é»˜è®¤éªŒè¯é€šè¿‡
            confidence_score=0.0,
            iteration_count=0,
            errors=[],
            messages=[]
        )

        # æ‰§è¡Œå­—æ®µæ˜ å°„å›¾
        try:
            # è®¾ç½®é€’å½’é™åˆ¶é…ç½®å’ŒLangSmithæ ‡ç­¾
            config = {
                "recursion_limit": 50,
                "tags": ["field-mapping", "excel-processing", "llm-analysis"],
                "metadata": {
                    "excel_filename": request.excelFilename or "json_data",
                    "user_input": request.userInput[:100],  # æˆªå–å‰100å­—ç¬¦
                    "field_count": len(excel_data) if isinstance(excel_data, list) and excel_data else 0
                }
            }
            logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œå­—æ®µæ˜ å°„å›¾...")
            result = field_mapping_graph.invoke(initial_state, config=config)
            logger.info("âœ… å­—æ®µæ˜ å°„å›¾æ‰§è¡Œå®Œæˆ")
            
            # æå–ç»“æœ
            final_mapping = result.get("final_mapping", {})
            confidence_score = result.get("confidence_score", 0.0)
            validation_results = result.get("validation_results", {})
            errors = result.get("errors", [])
            
            # ç”Ÿæˆå“åº”
            response = {
                "success": True,
                "mapping": final_mapping,
                "confidence_score": confidence_score,
                "validation": validation_results,
                "errors": errors,
                "message": "å­—æ®µæ˜ å°„å®Œæˆ"
            }
            
            return JSONResponse(content=response)
            
        except Exception as graph_error:
            logger.error(f"å­—æ®µæ˜ å°„å›¾æ‰§è¡Œå¤±è´¥: {str(graph_error)}")
            raise HTTPException(status_code=500, detail=f"å­—æ®µæ˜ å°„æ‰§è¡Œå¤±è´¥: {str(graph_error)}")

    except Exception as e:
        logger.error(f"å¤„ç†å­—æ®µæ˜ å°„è¯·æ±‚æ—¶å‡ºé”™: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# å°è£…POSTè¯·æ±‚æ¥å£ï¼Œä¸å¤§æ¨¡å‹è¿›è¡Œé—®ç­”
@app.post("/greet/stream")
async def chat_completions(request: ChatCompletionRequest):
    # åˆ¤æ–­åˆå§‹åŒ–æ˜¯å¦å®Œæˆ
    if not graph:
        logger.error("æœåŠ¡æœªåˆå§‹åŒ–")
        raise HTTPException(status_code=500, detail="æœåŠ¡æœªåˆå§‹åŒ–")

    try:
        logger.info(f"æ”¶åˆ°èŠå¤©å®Œæˆè¯·æ±‚: {request}")

        query_prompt = request.userInput

        # query_prompt = request.messages[-1].content
        logger.info(f"ç”¨æˆ·é—®é¢˜æ˜¯: {query_prompt}")
        config = {"configurable": {"thread_id": "456" + "@@" + "456"}}
        # config = {"configurable": {"thread_id": request.userId+"@@"+request.conversationId}}
        logger.info(f"ç”¨æˆ·å½“å‰ä¼šè¯ä¿¡æ¯: {config}")

        # æ ¹æ®æ˜¯å¦æœ‰Excelæ•°æ®æˆ–Excelæ–‡ä»¶é€‰æ‹©ä¸åŒçš„æç¤ºè¯
        excel_json_str = None
        
        if request.excelData:
            # ä½¿ç”¨è¯·æ±‚ä¸­çš„Excelæ•°æ®
            logger.info("æ£€æµ‹åˆ°Excelæ•°æ®ï¼Œä½¿ç”¨test_promptè¿›è¡Œåˆ†æ")
            excel_json_str = request.excelData
        elif request.excelFilename:
            # ä»Excelæ–‡ä»¶è¯»å–æ•°æ®
            logger.info(f"æ£€æµ‹åˆ°Excelæ–‡ä»¶: {request.excelFilename}ï¼Œä½¿ç”¨test_promptè¿›è¡Œåˆ†æ")
            try:
                excel_data = excel_processor.read_excel_to_json(request.excelFilename)
                excel_json_str = json.dumps(excel_data["data"], ensure_ascii=False)
                logger.info(f"æˆåŠŸè¯»å–Excelæ–‡ä»¶ï¼Œå…±{excel_data['total_rows']}è¡Œæ•°æ®")
            except FileNotFoundError:
                raise HTTPException(status_code=404, detail=f"Excelæ–‡ä»¶ä¸å­˜åœ¨: {request.excelFilename}")
            except Exception as e:
                logger.error(f"è¯»å–Excelæ–‡ä»¶å¤±è´¥: {str(e)}")
                raise HTTPException(status_code=500, detail=f"è¯»å–Excelæ–‡ä»¶å¤±è´¥: {str(e)}")
        
        if excel_json_str:
            # ä½¿ç”¨test_promptå¤„ç†Excelæ•°æ®
            try:
                system_template = test_prompt.messages[0].prompt.template
                user_template = user_prompt.messages[0].prompt.template
                
                # æ ¼å¼åŒ–ç³»ç»Ÿæç¤ºè¯
                formatted_system_content = system_template.format(excel_data=excel_json_str)
                logger.info(f"ç³»ç»Ÿæç¤ºè¯æ ¼å¼åŒ–æˆåŠŸï¼Œé•¿åº¦: {len(formatted_system_content)}")
                
                # æ ¼å¼åŒ–ç”¨æˆ·æç¤ºè¯
                formatted_user_content = user_template.format(query=query_prompt)
                logger.info(f"ç”¨æˆ·æç¤ºè¯æ ¼å¼åŒ–æˆåŠŸï¼Œé•¿åº¦: {len(formatted_user_content)}")
                
                prompt = [
                    {"role": "system", "content": formatted_system_content},
                    {"role": "user", "content": formatted_user_content}
                ]
                logger.info("æç¤ºè¯ç»„è£…æˆåŠŸ")
            except Exception as format_error:
                logger.error(f"æç¤ºè¯æ ¼å¼åŒ–å¤±è´¥: {str(format_error)}")
                raise HTTPException(status_code=500, detail=f"æç¤ºè¯æ ¼å¼åŒ–å¤±è´¥: {str(format_error)}")
        else:
            # ä½¿ç”¨greeting_card_promptè¿›è¡Œæ™®é€šå¯¹è¯
            logger.info("æœªæ£€æµ‹åˆ°Excelæ•°æ®ï¼Œä½¿ç”¨greeting_card_promptè¿›è¡Œæ™®é€šå¯¹è¯")
            greeting_card_template = greeting_card_prompt.messages[0].prompt.template
            user_template = user_prompt.messages[0].prompt.template

            prompt = [
                {"role": "system", "content": greeting_card_template},
                {"role": "user", "content": user_template.format(query=query_prompt)}
            ]

        # prompt_template_system = PromptTemplate.from_file(PROMPT_TEMPLATE_TXT_SYS)
        # prompt_template_user = PromptTemplate.from_file(PROMPT_TEMPLATE_TXT_USER)
        # prompt = [
        #     {"role": "system", "content": prompt_template_system.template},
        #     {"role": "user", "content": prompt_template_user.template.format(query=query_prompt)}
        # ]

        # å¤„ç†æµå¼å“åº”
        if request.stream:
            async def generate_stream():

                try:
                    chunk_id = f"chatcmpl-{uuid.uuid4().hex}"
                    async for message_chunk, metadata in graph.astream({"messages": prompt}, config, stream_mode="messages"):
                        try:
                            chunk = message_chunk.content
                            logger.info(f"chunk: {chunk}")

                            # ç»„è£…æ•°æ®
                            # choice = Choice(0, delta={'content': chunk}, finish_reason='null')
                            modelResponseData = ModelResponseData(id=chunk_id,
                                                                  object='chat.completion.chunk',
                                                                  created=current_time,
                                                                  content=chunk,
                                                                  finishReason='null'
                                                                  )
                            modelResponse = ModelResponse(data=modelResponseData)
                            modelResponse = json.loads(modelResponse.to_json())
                            dict_str = {
                                'code': 200,
                                'message': 'success',
                                **modelResponse
                            }
                            json_str = json.dumps(dict_str, ensure_ascii=False)
                            logger.info(f"å‘é€æ•°æ®json_str:{json_str}")
                            yield f"data: {json_str}\n\n".encode('utf-8')
                        except Exception as chunk_error:
                            # è®°å½•å•ä¸ªæ•°æ®å—å¤„ç†å¼‚å¸¸
                            logger.error(f"Error processing stream chunk: {chunk_error}")
                            continue
                        # åŸå§‹å¤„ç†è¿‡ç¨‹ï¼šåœ¨å¤„ç†è¿‡ç¨‹ä¸­äº§ç”Ÿæ¯ä¸ªå—
                        # yield f"data: {json.dumps({'id': chunk_id,'object': 'chat.completion.chunk','created': int(time.time()),'choices': [{'index': 0,'delta': {'content': chunk},'finish_reason': None}]})}\n\n"
                    # äº§å‡ºæµç»“æŸæ ‡è®°
                    response_data_end = {
                        'code': 200,
                        'message': 'success',
                        'data': {
                            'id': chunk_id,
                            'object': 'chat.completion.chunk',
                            'created': current_time,
                            'content': '',
                            'finishReason': 'stop'
                        }
                    }
                    json_str_end = json.dumps(response_data_end, ensure_ascii=False)
                    logger.info(f"end: {json_str_end}")
                    yield f"data: {json_str_end}\n\n"
                    # åŸå§‹å¤„ç†è¿‡ç¨‹ï¼šæµç»“æŸçš„æœ€åä¸€å—
                    # yield f"data: {json.dumps({'id': chunk_id,'object': 'chat.completion.chunk','created': int(time.time()),'choices': [{'index': 0,'delta': {},'finish_reason': 'stop'}]})}\n\n"
                except Exception as stream_error:
                    # è®°å½•æµç”Ÿæˆè¿‡ç¨‹ä¸­çš„å¼‚å¸¸
                    logger.error(f"Stream generation error: {stream_error}")
                    # äº§å‡ºé”™è¯¯æç¤º
                    yield f"data: {json.dumps({'error': 'Stream processing failed'})}\n\n"
            # è¿”å›fastapi.responsesä¸­StreamingResponseå¯¹è±¡
            return StreamingResponse(generate_stream(), media_type="text/event-stream")

        # å¤„ç†éæµå¼å“åº”å¤„ç†
        else:
            try:
                events = graph.stream({"messages": prompt}, config)
                for event in events:
                    for value in event.values():
                        result = value["messages"][-1].content
            except Exception as e:
                logger.info(f"Error processing response: {str(e)}")

            formatted_response = str(format_response(result))
            logger.info(f"æ ¼å¼åŒ–çš„æœç´¢ç»“æœ: {formatted_response}")

            response = ChatCompletionResponse(
                choices=[
                    ChatCompletionResponseChoice(
                        index=0,
                        message=Message(role="assistant", content=formatted_response),
                        finish_reason="stop"
                    )
                ]
            )
            logger.info(f"å‘é€å“åº”å†…å®¹: \n{response}")
            # è¿”å›fastapi.responsesä¸­JSONResponseå¯¹è±¡
            # model_dump()æ–¹æ³•é€šå¸¸ç”¨äºå°†Pydanticæ¨¡å‹å®ä¾‹çš„å†…å®¹è½¬æ¢ä¸ºä¸€ä¸ªæ ‡å‡†çš„Pythonå­—å…¸ï¼Œä»¥ä¾¿è¿›è¡Œåºåˆ—åŒ–
            return JSONResponse(content=response.model_dump())

    except Exception as e:
        logger.error(f"å¤„ç†èŠå¤©å®Œæˆæ—¶å‡ºé”™:\n\n {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))



if __name__ == "__main__":
    logger.info(f"åœ¨ç«¯å£ {app_port} ä¸Šå¯åŠ¨æœåŠ¡å™¨")
    # uvicornæ˜¯ä¸€ä¸ªç”¨äºè¿è¡ŒASGIåº”ç”¨çš„è½»é‡çº§ã€è¶…å¿«é€Ÿçš„ASGIæœåŠ¡å™¨å®ç°
    # ç”¨äºéƒ¨ç½²åŸºäºFastAPIæ¡†æ¶çš„å¼‚æ­¥PythonWebåº”ç”¨ç¨‹åº
    uvicorn.run(app, host=app_host, port=app_port)


