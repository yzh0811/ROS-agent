import json
import logging
import re
import unicodedata
from typing import List, Optional, Dict, Any
from typing import Annotated
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

logger = logging.getLogger(__name__)


class FieldMappingState(TypedDict):
    excel_data: Dict[str, Any]
    preprocessed_fields: List[Dict[str, Any]]
    classified_fields: List[Dict[str, Any]]
    initial_mapping: Dict[str, str]
    final_mapping: Dict[str, str]
    validation_results: Dict[str, Any]
    confidence_score: float
    iteration_count: int
    errors: List[str]
    messages: Annotated[list, add_messages]


def parse_llm_response_robust(result_content: str) -> Dict[str, str]:
    try:
        standard_fields = [
            "è¿è¾“æ—¥æœŸ", "è®¢å•å·", "è·¯é¡º", "æ‰¿è¿å•†", "è¿å•å·", "è½¦åž‹",
            "å‘è´§æ–¹ç¼–å·", "å‘è´§æ–¹åç§°", "æ”¶è´§æ–¹ç¼–ç ", "æ”¶è´§æ–¹åç§°",
            "å•†å“ç¼–ç ", "å•†å“åç§°", "ä»¶æ•°", "ä½“ç§¯", "é‡é‡"
        ]
        json_patterns = [r'\{.*\}', r'```json\s*(\{.*?\})\s*```', r'```\s*(\{.*?\})\s*```']
        for pattern in json_patterns:
            json_match = re.search(pattern, result_content, re.DOTALL)
            if json_match:
                try:
                    json_str = json_match.group(1) if len(json_match.groups()) > 0 else json_match.group(0)
                    llm_result = json.loads(json_str)
                    if isinstance(llm_result, dict):
                        if "æ˜ å°„ç»“æžœ" in llm_result and isinstance(llm_result["æ˜ å°„ç»“æžœ"], dict):
                            return llm_result["æ˜ å°„ç»“æžœ"]
                        elif "mapping" in llm_result and isinstance(llm_result["mapping"], dict):
                            return llm_result["mapping"]
                        elif any(k in llm_result for k in standard_fields):
                            return {k: llm_result.get(k, "missing") for k in standard_fields}
                        elif "confidence" in llm_result or "å‡†ç¡®çŽ‡" in llm_result:
                            return extract_mapping_from_text(result_content, standard_fields)
                except (json.JSONDecodeError, KeyError):
                    continue
        return extract_mapping_from_text(result_content, standard_fields)
    except Exception as e:
        logger.error(f"LLMå“åº”è§£æžå¤±è´¥: {str(e)}")
        return {}


def apply_similarity_fill(initial_mapping: dict, preprocessed_fields: list, threshold: float = 0.4, only_missing: bool = True) -> dict:
    def _norm(s: str) -> str:
        if s is None:
            return ""
        t = unicodedata.normalize("NFKC", str(s)).lower()
        t = re.sub(r"[_\-\./\\]+", " ", t)
        return re.sub(r"\s+", " ", t).strip()

    def _tok(s: str):
        return re.findall(r"[a-z0-9]+|[\u4e00-\u9fa5]+", _norm(s))

    def _edit_sim(a: str, b: str) -> float:
        a, b = _norm(a), _norm(b)
        if not a and not b:
            return 1.0
        if not a or not b:
            return 0.0
        la, lb = len(a), len(b)
        dp = [[0] * (lb + 1) for _ in range(la + 1)]
        for i in range(la + 1):
            dp[i][0] = i
        for j in range(lb + 1):
            dp[0][j] = j
        for i in range(1, la + 1):
            for j in range(1, lb + 1):
                dp[i][j] = min(
                    dp[i - 1][j] + 1,
                    dp[i][j - 1] + 1,
                    dp[i - 1][j - 1] + (a[i - 1] != b[j - 1]),
                )
        dist = dp[la][lb]
        return 1 - dist / max(1, max(la, lb))

    def _jaccard(a: str, b: str) -> float:
        sa, sb = set(_tok(a)), set(_tok(b))
        return (len(sa & sb) / len(sa | sb)) if sa and sb else 0.0

    UNIT_HINTS = {
        "ä½“ç§¯": ["cbm", "m3", "mÂ³", "ç«‹æ–¹", "å®¹ç§¯", "cubic", "vol"],
        "é‡é‡": ["kg", "å…¬æ–¤", "åƒå…‹", "å¨", "lb", "wt", "weight"],
        "ä»¶æ•°": ["pcs", "box", "qty", "count", "ç®±", "åŒ…", "æ•°é‡", "ä»¶"],
    }
    TOTAL_HINTS = ["æ€»", "åˆè®¡", "æ€»è®¡", "total", "sum"]

    def _bonus(name: str, field: str) -> float:
        n = _norm(name)
        b = 0.0
        if field in UNIT_HINTS and any(h in n for h in UNIT_HINTS[field]):
            b += 0.2
        if field in ["ä»¶æ•°", "ä½“ç§¯", "é‡é‡"] and any(h in n for h in TOTAL_HINTS):
            b += 0.2
        return min(b, 0.4)

    std_fields = [
        "è¿è¾“æ—¥æœŸ", "è®¢å•å·", "è·¯é¡º", "æ‰¿è¿å•†", "è¿å•å·", "è½¦åž‹",
        "å‘è´§æ–¹ç¼–å·", "å‘è´§æ–¹åç§°", "æ”¶è´§æ–¹ç¼–ç ", "æ”¶è´§æ–¹åç§°",
        "å•†å“ç¼–ç ", "å•†å“åç§°", "ä»¶æ•°", "ä½“ç§¯", "é‡é‡",
    ]
    if not preprocessed_fields:
        return initial_mapping
    cols = [f.get("name") for f in preprocessed_fields if f.get("name")]
    used = set(v for v in initial_mapping.values() if v and v != "missing")
    updated = dict(initial_mapping)

    for f in std_fields:
        if only_missing and updated.get(f) and updated[f] != "missing":
            continue
        best_name, best_score = None, 0.0
        for name in cols:
            if name in used:
                continue
            score = 0.6 * _edit_sim(name, f) + 0.4 * _jaccard(name, f) + _bonus(name, f)
            if score > best_score:
                best_score, best_name = score, name
        if best_name and best_score >= threshold:
            updated[f] = best_name
            used.add(best_name)
    return updated


def extract_mapping_from_text(text: str, standard_fields: List[str]) -> Dict[str, str]:
    try:
        mapping = {field: "missing" for field in standard_fields}
        for field in standard_fields:
            field_pattern = rf'{field}[ï¼š:]\s*["""]?([^"""\n]+)["""]?'
            match = re.search(field_pattern, text)
            if match:
                mapped_field = match.group(1).strip()
                if mapped_field and mapped_field != "missing":
                    mapping[field] = mapped_field
        return mapping
    except Exception as e:
        logger.error(f"æ–‡æœ¬æå–æ˜ å°„å¤±è´¥: {str(e)}")
        return {field: "missing" for field in standard_fields}


def try_hybrid_mapping(state: FieldMappingState, llm_response: str) -> Dict[str, str]:
    try:
        standard_fields = [
            "è¿è¾“æ—¥æœŸ", "è®¢å•å·", "è·¯é¡º", "æ‰¿è¿å•†", "è¿å•å·", "è½¦åž‹",
            "å‘è´§æ–¹ç¼–å·", "å‘è´§æ–¹åç§°", "æ”¶è´§æ–¹ç¼–ç ", "æ”¶è´§æ–¹åç§°",
            "å•†å“ç¼–ç ", "å•†å“åç§°", "ä»¶æ•°", "ä½“ç§¯", "é‡é‡",
        ]
        hybrid_mapping = {field: "missing" for field in standard_fields}
        partial_mapping = extract_mapping_from_text(llm_response, standard_fields)
        rule_mapping = get_rule_based_mapping(state)
        for field in standard_fields:
            if partial_mapping.get(field) != "missing":
                hybrid_mapping[field] = partial_mapping[field]
            elif rule_mapping.get(field) != "missing":
                hybrid_mapping[field] = rule_mapping[field]
        logger.info(f"ðŸ” [æ··åˆç­–ç•¥] æˆåŠŸæ˜ å°„ {len([v for v in hybrid_mapping.values() if v != 'missing'])} ä¸ªå­—æ®µ")
        return hybrid_mapping
    except Exception as e:
        logger.error(f"æ··åˆç­–ç•¥å¤±è´¥: {str(e)}")
        return {field: "missing" for field in standard_fields}


def get_rule_based_mapping(state: FieldMappingState) -> Dict[str, str]:
    try:
        classified_fields = state.get("classified_fields", [])
        standard_fields = [
            "è¿è¾“æ—¥æœŸ", "è®¢å•å·", "è·¯é¡º", "æ‰¿è¿å•†", "è¿å•å·", "è½¦åž‹",
            "å‘è´§æ–¹ç¼–å·", "å‘è´§æ–¹åç§°", "æ”¶è´§æ–¹ç¼–ç ", "æ”¶è´§æ–¹åç§°",
            "å•†å“ç¼–ç ", "å•†å“åç§°", "ä»¶æ•°", "ä½“ç§¯", "é‡é‡",
        ]
        rule_mapping = {}
        used_fields = set()
        for standard_field in standard_fields:
            best_match = "missing"
            best_score = 0
            for field in classified_fields:
                if field["name"] in used_fields:
                    continue
                score = calculate_mapping_score(standard_field, field)
                if score > best_score:
                    best_score = score
                    best_match = field["name"]
            if best_score > 0.3:
                rule_mapping[standard_field] = best_match
                used_fields.add(best_match)
            else:
                rule_mapping[standard_field] = "missing"
        return rule_mapping
    except Exception as e:
        logger.error(f"è§„åˆ™æ˜ å°„èŽ·å–å¤±è´¥: {str(e)}")
        return {field: "missing" for field in standard_fields}


def preprocess_fields(state: FieldMappingState) -> FieldMappingState:
    try:
        excel_data = state["excel_data"]
        preprocessed_fields = []
        if isinstance(excel_data, list) and len(excel_data) > 0:
            if isinstance(excel_data[0], dict):
                columns = list(excel_data[0].keys())
                converted_data = {}
                for col in columns:
                    converted_data[col] = [row.get(col) for row in excel_data]
                excel_data = converted_data
        for column_name, column_data in excel_data.items():
            if not isinstance(column_data, list):
                continue
            total_rows = len(column_data)
            non_null_count = sum(1 for value in column_data if value is not None and value != "")
            null_rate = (total_rows - non_null_count) / total_rows if total_rows > 0 else 1.0
            shipper_keywords = [
                "ç«™ç‚¹", "ç½‘ç‚¹", "é—¨åº—", "ä»“åº“", "é…é€ä¸­å¿ƒ", "DC", "å·¥åŽ‚",
                "ç«™å", "ç½‘å", "åº—å", "ä»“å", "åŽ‚å",
                "ç«™å·", "ç½‘å·", "åº—å·", "ä»“å·", "åŽ‚å·",
                "ç«™ç¼–ç ", "ç½‘ç¼–ç ", "åº—ç¼–ç ", "ä»“ç¼–ç ", "åŽ‚ç¼–ç ",
                "å‘è´§", "å‘é€", "å§‹å‘", "èµ·è¿", "å‘è¿",
            ]
            is_shipper_field = any(keyword in column_name for keyword in shipper_keywords)
            if null_rate > 0.9 and not is_shipper_field:
                continue
            elif is_shipper_field and null_rate > 0.9:
                logger.info(f"ðŸ” [é¢„å¤„ç†] å‘è´§æ–¹å­—æ®µè¢«ä¿æŠ¤: {column_name} (ç©ºå€¼çŽ‡: {null_rate:.2%})")
            unique_values = set(str(v) for v in column_data if v is not None and v != "")
            uniqueness = len(unique_values) / total_rows if total_rows > 0 else 0
            data_type = "text"
            if all(isinstance(v, (int, float)) for v in column_data if v is not None):
                data_type = "numeric"
            elif all(isinstance(v, str) and len(v) == 10 for v in column_data if v is not None):
                data_type = "date"
            field_info = {
                "name": column_name,
                "data": column_data,
                "null_rate": null_rate,
                "uniqueness": uniqueness,
                "data_type": data_type,
                "total_rows": total_rows,
                "non_null_count": non_null_count,
            }
            preprocessed_fields.append(field_info)
        state["preprocessed_fields"] = preprocessed_fields
        state["messages"].append({"role": "system", "content": f"å­—æ®µé¢„å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(preprocessed_fields)} ä¸ªæœ‰æ•ˆå­—æ®µ"})
        return state
    except Exception as e:
        state["errors"].append(f"å­—æ®µé¢„å¤„ç†å¤±è´¥: {str(e)}")
        return state


def classify_fields(state: FieldMappingState) -> FieldMappingState:
    try:
        preprocessed_fields = state["preprocessed_fields"]
        classified_fields = []
        for field in preprocessed_fields:
            field_name = field["name"].lower()
            data_type = field["data_type"]
            uniqueness = field["uniqueness"]
            category = "unknown"
            if any(keyword in field_name for keyword in ["æ—¶é—´", "æ—¥æœŸ", "date", "time"]) or data_type == "date":
                category = "date"
            elif any(keyword in field_name for keyword in ["ç¼–å·", "ç¼–ç ", "code", "id", "å·"]) and uniqueness > 0.8:
                category = "identifier"
            elif any(keyword in field_name for keyword in ["æ•°é‡", "ä»¶æ•°", "ä½“ç§¯", "é‡é‡", "count", "volume", "weight"]) and data_type == "numeric":
                category = "quantity"
            elif any(keyword in field_name for keyword in ["åç§°", "name", "åœ°å€", "address", "å…¬å¸", "company"]):
                category = "name"
            field["category"] = category
            classified_fields.append(field)
        state["classified_fields"] = classified_fields
        state["messages"].append({"role": "system", "content": f"å­—æ®µåˆ†ç±»å®Œæˆï¼Œå…±åˆ†ç±» {len(classified_fields)} ä¸ªå­—æ®µ"})
        return state
    except Exception as e:
        state["errors"].append(f"å­—æ®µåˆ†ç±»å¤±è´¥: {str(e)}")
        return state


def llm_map_to_standard_fields(state: FieldMappingState) -> FieldMappingState:
    try:
        from src.prompts.prompts import test_prompt
        from src.configs.model_init import chat_model
        excel_data = state["excel_data"]
        excel_json_str = json.dumps(excel_data, ensure_ascii=False)
        try:
            system_template = test_prompt.messages[0].prompt.template
            formatted_system_content = system_template.format(excel_data=excel_json_str)
            prompt = [{"role": "system", "content": formatted_system_content}]
            response = chat_model.invoke(prompt)
            result_content = response.content
            try:
                initial_mapping = parse_llm_response_robust(result_content)
                if initial_mapping:
                    state["initial_mapping"] = initial_mapping
                    state["confidence_score"] = 85.0
                    state["messages"].append({
                        "role": "assistant",
                        "content": f"LLMå­—æ®µæ˜ å°„å®Œæˆï¼Œæ˜ å°„äº† {len([v for v in initial_mapping.values() if v != 'missing'])} ä¸ªå­—æ®µï¼Œç½®ä¿¡åº¦: 85%",
                    })
                    try:
                        pre_fields = state.get("preprocessed_fields", [])
                        sim_updated = apply_similarity_fill(state["initial_mapping"], pre_fields, threshold=0.4, only_missing=True)
                        if sim_updated != state["initial_mapping"]:
                            state["initial_mapping"] = sim_updated
                    except Exception:
                        pass
                else:
                    initial_mapping = try_hybrid_mapping(state, result_content)
                    state["initial_mapping"] = initial_mapping
                    state["messages"].append({
                        "role": "system",
                        "content": f"æ··åˆç­–ç•¥å®Œæˆï¼Œæ˜ å°„äº† {len([v for v in initial_mapping.values() if v != 'missing'])} ä¸ªå­—æ®µ",
                    })
            except Exception as parse_error:
                state["errors"].append(f"LLMå“åº”å¤„ç†å¤±è´¥: {str(parse_error)}")
                return rule_based_mapping_fallback(state)
        except Exception as llm_error:
            state["errors"].append(f"LLMè°ƒç”¨å¤±è´¥: {str(llm_error)}")
            return rule_based_mapping_fallback(state)
        return state
    except Exception as e:
        state["errors"].append(f"LLMå­—æ®µåŒ¹é…å¤±è´¥: {str(e)}")
        return rule_based_mapping_fallback(state)


def rule_based_mapping_fallback(state: FieldMappingState) -> FieldMappingState:
    try:
        logger.info("ðŸ” [è§„åˆ™é™çº§] å¼€å§‹è§„åˆ™åŒ¹é…é™çº§")
        initial_mapping = get_rule_based_mapping(state)
        try:
            from src.configs.manual_mapping_rules import get_mapping_rules
            mapping_rules = get_mapping_rules()
            preprocessed_fields = state.get("preprocessed_fields", [])
            available_fields = [field["name"] for field in preprocessed_fields]
            logger.info(f"ðŸ” [è§„åˆ™å›žé€€] å¯ç”¨å­—æ®µ: {available_fields}")
            for standard_field, keywords in mapping_rules.items():
                if initial_mapping.get(standard_field) == "missing":
                    for field_name in available_fields:
                        if any(keyword in field_name for keyword in keywords):
                            initial_mapping[standard_field] = field_name
                            logger.info(f"ðŸ” [è§„åˆ™å›žé€€] æ™ºèƒ½è¡¥å…… {standard_field}: {field_name}")
                            break
        except ImportError:
            logger.warning("âš ï¸ [è§„åˆ™å›žé€€] æ— æ³•å¯¼å…¥æ‰‹åŠ¨æ˜ å°„è§„åˆ™ï¼Œä½¿ç”¨å†…ç½®è§„åˆ™")
            preprocessed_fields = state.get("preprocessed_fields", [])
            if initial_mapping.get("å‘è´§æ–¹ç¼–å·") == "missing" or initial_mapping.get("å‘è´§æ–¹åç§°") == "missing":
                logger.info("ðŸ” [è§„åˆ™å›žé€€] å°è¯•æ™ºèƒ½è¡¥å……å‘è´§æ–¹æ˜ å°„...")
                shipper_id_keywords = ["ç«™ç‚¹ç¼–å·", "ç½‘ç‚¹ç¼–å·", "é—¨åº—ç¼–å·", "ä»“åº“ç¼–å·", "é…é€ä¸­å¿ƒç¼–å·", "DCç¼–å·", "å·¥åŽ‚ç¼–å·", "ç«™å·", "ç½‘å·", "åº—å·", "ä»“å·", "åŽ‚å·"]
                shipper_name_keywords = ["ç«™ç‚¹åç§°", "ç½‘ç‚¹åç§°", "é—¨åº—åç§°", "ä»“åº“åç§°", "é…é€ä¸­å¿ƒåç§°", "DCåç§°", "å·¥åŽ‚åç§°", "ç«™å", "ç½‘å", "åº—å", "ä»“å", "åŽ‚å"]
                if initial_mapping.get("å‘è´§æ–¹ç¼–å·") == "missing":
                    for field in preprocessed_fields:
                        field_name = field["name"]
                        if any(keyword in field_name for keyword in shipper_id_keywords):
                            initial_mapping["å‘è´§æ–¹ç¼–å·"] = field_name
                            logger.info(f"ðŸ” [è§„åˆ™å›žé€€] æ™ºèƒ½è¡¥å……å‘è´§æ–¹ç¼–å·: {field_name}")
                            break
                if initial_mapping.get("å‘è´§æ–¹åç§°") == "missing":
                    for field in preprocessed_fields:
                        field_name = field["name"]
                        if any(keyword in field_name for keyword in shipper_name_keywords):
                            initial_mapping["å‘è´§æ–¹åç§°"] = field_name
                            logger.info(f"ðŸ” [è§„åˆ™å›žé€€] æ™ºèƒ½è¡¥å……å‘è´§æ–¹åç§°: {field_name}")
                            break
        try:
            pre_fields = state.get("preprocessed_fields", [])
            sim_updated = apply_similarity_fill(initial_mapping, pre_fields, threshold=0.4, only_missing=True)
            if sim_updated != initial_mapping:
                initial_mapping = sim_updated
        except Exception:
            pass
        state["initial_mapping"] = initial_mapping
        state["messages"].append({"role": "system", "content": f"è§„åˆ™åŒ¹é…é™çº§å®Œæˆï¼Œæ˜ å°„äº† {len([v for v in initial_mapping.values() if v != 'missing'])} ä¸ªå­—æ®µ"})
        return state
    except Exception as e:
        state["errors"].append(f"è§„åˆ™åŒ¹é…é™çº§å¤±è´¥: {str(e)}")
        return state


def calculate_mapping_score(standard_field: str, field: Dict[str, Any]) -> float:
    score = 0.0
    field_name = field["name"].lower()
    if standard_field == "è®¢å•å·":
        if any(keyword in field_name for keyword in ["è®¢å•", "å•å·", "order", "è®¢å•å·"]):
            score += 0.6
        if field["uniqueness"] > 0.9:
            score += 0.4
    elif standard_field == "è¿å•å·":
        if any(keyword in field_name for keyword in ["è¿å•", "è¿å•å·", "waybill", "è¿å•å·"]):
            score += 0.6
        if field["uniqueness"] > 0.7:
            score += 0.3
    elif standard_field == "è¿è¾“æ—¥æœŸ":
        if any(keyword in field_name for keyword in ["è¿è¾“", "å‘è¿", "æ—¥æœŸ", "æ—¶é—´", "date", "time"]):
            score += 0.6
        if field["data_type"] == "date":
            score += 0.4
    elif standard_field == "ä»¶æ•°":
        if any(keyword in field_name for keyword in ["ä»¶æ•°", "æ•°é‡", "count", "qty", "ä»¶", "æ€»ä»¶æ•°", "ä»¶æ•°åˆè®¡", "ä»¶æ•°æ€»è®¡", "ç®±æ•°", "æ€»ç®±æ•°", "åŒ…æ•°", "æ€»åŒ…æ•°", "pcs", "box"]):
            score += 0.8
        if field["data_type"] == "numeric":
            score += 0.2
        if any(keyword in field_name for keyword in ["æ€»", "åˆè®¡", "æ€»è®¡", "total", "sum"]):
            score += 0.3
    elif standard_field == "ä½“ç§¯":
        if any(keyword in field_name for keyword in ["ä½“ç§¯", "volume", "ç«‹æ–¹", "m3", "æ€»ä½“ç§¯", "ä½“ç§¯åˆè®¡", "ä½“ç§¯æ€»è®¡", "å®¹ç§¯", "ç©ºé—´", "ä½“ç§¯é‡", "vol", "cbm", "cubic"]):
            score += 0.8
        if field["data_type"] == "numeric":
            score += 0.2
        if any(keyword in field_name for keyword in ["æ€»", "åˆè®¡", "æ€»è®¡", "total", "sum"]):
            score += 0.3
    elif standard_field == "é‡é‡":
        if any(keyword in field_name for keyword in ["é‡é‡", "weight", "å…¬æ–¤", "kg", "é‡", "æ€»é‡é‡", "é‡é‡åˆè®¡", "é‡é‡æ€»è®¡", "åƒå…‹", "å¨", "æ–¤", "ç£…", "wt", "ton", "lb"]):
            score += 0.8
        if field["data_type"] == "numeric":
            score += 0.2
        if any(keyword in field_name for keyword in ["æ€»", "åˆè®¡", "æ€»è®¡", "total", "sum"]):
            score += 0.3
    elif standard_field == "æ‰¿è¿å•†":
        if any(keyword in field_name for keyword in [
            "æ‰¿è¿å•†", "ç‰©æµ", "è¿è¾“", "carrier", "logistics",
            "è¿è¾“å…¬å¸", "ç‰©æµå…¬å¸", "å¿«é€’å…¬å¸", "é…é€å…¬å¸",
            "ä¾›åº”å•†", "åˆ†åŒ…å•†", "æŒ‡å®šæ‰¿è¿å•†", "åˆä½œæ‰¿è¿å•†",
        ]):
            score += 0.7
        if field["category"] == "name":
            score += 0.3
    elif standard_field == "è½¦åž‹":
        if any(keyword in field_name for keyword in ["è½¦åž‹", "è½¦è¾†", "vehicle", "truck", "è½¦"]):
            score += 0.8
        if field["category"] == "name":
            score += 0.2
    elif standard_field == "å‘è´§æ–¹åç§°":
        if any(keyword in field_name for keyword in [
            "å‘è´§", "å‘é€", "sender", "shipper", "å‘",
            "ç«™ç‚¹", "ç½‘ç‚¹", "é—¨åº—", "ä»“åº“", "é…é€ä¸­å¿ƒ", "DC", "å·¥åŽ‚",
            "ç«™å", "ç½‘å", "åº—å", "ä»“å", "åŽ‚å",
            "å‘è´§ç‚¹", "å§‹å‘ç‚¹", "èµ·è¿ç‚¹", "å‘è¿ç‚¹",
        ]):
            score += 0.7
        if field["category"] == "name":
            score += 0.3
    elif standard_field == "æ”¶è´§æ–¹åç§°":
        if any(keyword in field_name for keyword in ["æ”¶è´§", "æŽ¥æ”¶", "receiver", "consignee", "æ”¶"]):
            score += 0.7
        if field["category"] == "name":
            score += 0.3
    elif standard_field == "å‘è´§æ–¹ç¼–å·":
        if any(keyword in field_name for keyword in ["å‘è´§", "å‘é€", "sender", "shipper", "å‘"]) and any(keyword in field_name for keyword in ["ç¼–å·", "ç¼–ç ", "code", "id"]):
            score += 0.8
        elif any(keyword in field_name for keyword in [
            "ç«™ç‚¹ç¼–å·", "ç½‘ç‚¹ç¼–å·", "é—¨åº—ç¼–å·", "ä»“åº“ç¼–å·", "é…é€ä¸­å¿ƒç¼–å·", "DCç¼–å·", "å·¥åŽ‚ç¼–å·",
            "ç«™å·", "ç½‘å·", "åº—å·", "ä»“å·", "åŽ‚å·", "é…é€å·",
        ]):
            score += 0.9
        elif any(keyword in field_name for keyword in [
            "ç«™ç‚¹ç¼–ç ", "ç½‘ç‚¹ç¼–ç ", "é—¨åº—ç¼–ç ", "ä»“åº“ç¼–ç ", "é…é€ä¸­å¿ƒç¼–ç ", "DCç¼–ç ", "å·¥åŽ‚ç¼–ç ",
            "ç«™ç¼–ç ", "ç½‘ç¼–ç ", "åº—ç¼–ç ", "ä»“ç¼–ç ", "åŽ‚ç¼–ç ",
        ]):
            score += 0.9
        if field["category"] == "identifier":
            score += 0.2
    elif standard_field == "æ”¶è´§æ–¹ç¼–ç ":
        if any(keyword in field_name for keyword in ["æ”¶è´§", "æŽ¥æ”¶", "receiver"]) and any(keyword in field_name for keyword in ["ç¼–å·", "ç¼–ç ", "code", "id"]):
            score += 0.8
        elif any(keyword in field_name for keyword in [
            "é€è´§ç‚¹", "é€è´§ç‚¹ç¼–å·", "é…é€ç‚¹", "é…é€ç‚¹ç¼–å·", "ç›®çš„åœ°", "ç›®çš„åœ°ç¼–å·", "å®¢æˆ·", "å®¢æˆ·ç¼–å·", "å®¢æˆ·å·", "ç»ˆç«¯", "ç»ˆç«¯ç¼–å·",
        ]):
            score += 0.9
        if any(keyword in field_name.lower() for keyword in ["é€è´§", "æ”¶è´§", "é…é€", "ç›®çš„åœ°", "å®¢æˆ·", "ç»ˆç«¯"]):
            score += 0.3
        if field["category"] == "identifier":
            score += 0.2
    elif standard_field == "æ”¶è´§æ–¹åç§°":
        if any(keyword in field_name for keyword in ["æ”¶è´§", "æŽ¥æ”¶", "receiver"]) and any(keyword in field_name for keyword in ["åç§°", "name", "æ”¶è´§æ–¹åç§°"]):
            score += 0.8
        elif any(keyword in field_name for keyword in [
            "é€è´§ç‚¹", "é€è´§ç‚¹åç§°", "é…é€ç‚¹", "é…é€ç‚¹åç§°", "ç›®çš„åœ°", "ç›®çš„åœ°åç§°", "å®¢æˆ·", "å®¢æˆ·åç§°", "å®¢æˆ·å", "ç»ˆç«¯", "ç»ˆç«¯åç§°",
        ]):
            score += 0.9
        if any(keyword in field_name.lower() for keyword in ["é€è´§", "æ”¶è´§", "é…é€", "ç›®çš„åœ°", "å®¢æˆ·", "ç»ˆç«¯"]):
            score += 0.3
        if field["category"] == "name":
            score += 0.2
    elif standard_field == "å•†å“ç¼–ç ":
        if any(keyword in field_name for keyword in ["å•†å“", "äº§å“", "product", "goods", "è´§ç‰©", "è´§å“"]) and any(keyword in field_name for keyword in ["ç¼–å·", "ç¼–ç ", "code", "id", "è´§å·", "å“å·"]):
            score += 0.8
        elif any(keyword in field_name for keyword in [
            "è´§å·", "å“å·", "SKU", "sku", "å•†å“ä»£ç ", "äº§å“ä»£ç ", "item id", "item id", "item code", "product code", "product id", "goods code", "material code",
        ]):
            score += 0.9
        if any(keyword in field_name.lower() for keyword in ["item", "product", "goods", "material", "sku"]):
            score += 0.3
        if field["category"] == "identifier":
            score += 0.2
    elif standard_field == "å•†å“åç§°":
        if any(keyword in field_name for keyword in ["å•†å“", "äº§å“", "product", "goods", "è´§ç‰©", "è´§å“"]) and any(keyword in field_name for keyword in ["åç§°", "name", "å“å", "è´§å"]):
            score += 0.8
        elif any(keyword in field_name for keyword in [
            "å“å", "è´§å", "å•†å“æè¿°", "äº§å“æè¿°", "è´§ç‰©æè¿°", "item description", "item desc", "product name", "product description", "goods name", "material description",
        ]):
            score += 0.9
        if any(keyword in field_name.lower() for keyword in ["item", "product", "goods", "material", "description", "desc", "name"]):
            score += 0.3
        if field["category"] == "name":
            score += 0.2
    elif standard_field == "è·¯é¡º":
        if any(keyword in field_name for keyword in ["è·¯é¡º", "é¡ºåº", "sequence", "order", "è·¯çº¿"]):
            score += 0.9
        if field["data_type"] == "numeric":
            score += 0.1
    return score


def resolve_conflicts_and_missing(state: FieldMappingState) -> FieldMappingState:
    try:
        initial_mapping = state.get("initial_mapping", {})
        logger.info(f"ðŸ” [å†²çªå¤„ç†] å¼€å§‹å¤„ç†å†²çªä¸Žç¼ºå¤±")
        logger.info(f"ðŸ” [å†²çªå¤„ç†] å†²çªå¤„ç†å‰çš„æ˜ å°„: {json.dumps(initial_mapping, ensure_ascii=False)}")
        if not initial_mapping:
            logger.warning("âš ï¸ initial_mappingä¸ºç©ºï¼Œåˆ›å»ºé»˜è®¤æ˜ å°„")
            standard_fields = [
                "è¿è¾“æ—¥æœŸ", "è®¢å•å·", "è·¯é¡º", "æ‰¿è¿å•†", "è¿å•å·", "è½¦åž‹",
                "å‘è´§æ–¹ç¼–å·", "å‘è´§æ–¹åç§°", "æ”¶è´§æ–¹ç¼–ç ", "æ”¶è´§æ–¹åç§°",
                "å•†å“ç¼–ç ", "å•†å“åç§°", "ä»¶æ•°", "ä½“ç§¯", "é‡é‡",
            ]
            initial_mapping = {field: "missing" for field in standard_fields}
        final_mapping = initial_mapping.copy()
        value_counts = {}
        for standard_field, personalized_field in initial_mapping.items():
            if personalized_field != "missing":
                if personalized_field in value_counts:
                    value_counts[personalized_field].append(standard_field)
                else:
                    value_counts[personalized_field] = [standard_field]
        if value_counts:
            logger.info(f"ðŸ” [å†²çªå¤„ç†] æ£€æµ‹åˆ°çš„å­—æ®µæ˜ å°„å…³ç³»:")
            for personalized_field, standard_fields in value_counts.items():
                if len(standard_fields) > 1:
                    logger.info(f"ðŸ” [å†²çªå¤„ç†] å†²çª: {personalized_field} -> {standard_fields}")
                else:
                    logger.info(f"ðŸ” [å†²çªå¤„ç†] æ­£å¸¸: {personalized_field} -> {standard_fields}")
        else:
            logger.info(f"ðŸ” [å†²çªå¤„ç†] æ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•å­—æ®µæ˜ å°„å…³ç³»")
        conflict_count = 0
        for personalized_field, standard_fields in value_counts.items():
            if len(standard_fields) > 1:
                conflict_count += 1
                logger.info(f"ðŸ” [å†²çªå¤„ç†] å¤„ç†ç¬¬{conflict_count}ä¸ªå†²çª: {personalized_field} -> {standard_fields}")
                priority_order = ["ä»¶æ•°", "ä½“ç§¯", "é‡é‡", "å•†å“ç¼–ç ", "å•†å“åç§°", "æ”¶è´§æ–¹ç¼–ç ", "æ”¶è´§æ–¹åç§°", "å‘è´§æ–¹ç¼–å·", "å‘è´§æ–¹åç§°", "è¿è¾“æ—¥æœŸ", "è®¢å•å·", "è¿å•å·"]
                field_scores = {}
                for field in standard_fields:
                    if field in priority_order:
                        score = priority_order.index(field)
                        field_scores[field] = score
                        logger.info(f"ðŸ” [å†²çªå¤„ç†] {field} ä¼˜å…ˆçº§åˆ†æ•°: {score}")
                    else:
                        field_scores[field] = 999
                        logger.info(f"ðŸ” [å†²çªå¤„ç†] {field} ä¼˜å…ˆçº§åˆ†æ•°: 999 (ä¸åœ¨ä¼˜å…ˆçº§åˆ—è¡¨ä¸­)")
                best_field = max(standard_fields, key=lambda x: priority_order.index(x) if x in priority_order else 999)
                logger.info(f"ðŸ” [å†²çªå¤„ç†] é€‰æ‹©ä¿ç•™: {best_field} (ä¼˜å…ˆçº§åˆ†æ•°: {field_scores[best_field]})")
                for field in standard_fields:
                    if field != best_field:
                        final_mapping[field] = "missing"
                        logger.info(f"ðŸ” [å†²çªå¤„ç†] è®¾ä¸ºmissing: {field} (ä¼˜å…ˆçº§åˆ†æ•°: {field_scores[field]})")
        if conflict_count == 0:
            logger.info(f"ðŸ” [å†²çªå¤„ç†] æ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•å†²çª")
        logger.info(f"ðŸ” [å†²çªå¤„ç†] å†²çªå¤„ç†åŽçš„æ˜ å°„: {json.dumps(final_mapping, ensure_ascii=False)}")
        shipper_fields = ["å‘è´§æ–¹åç§°", "å‘è´§æ–¹ç¼–å·"]
        for field in shipper_fields:
            if initial_mapping.get(field) != final_mapping.get(field):
                logger.warning(f"âš ï¸ [å†²çªå¤„ç†] {field} çŠ¶æ€å‘ç”Ÿå˜åŒ–: {initial_mapping.get(field)} -> {final_mapping.get(field)}")
        state["final_mapping"] = final_mapping
        state["messages"].append({"role": "system", "content": "å†²çªä¸Žç¼ºå¤±å¤„ç†å®Œæˆ"})
        return state
    except Exception as e:
        state["errors"].append(f"å†²çªä¸Žç¼ºå¤±å¤„ç†å¤±è´¥: {str(e)}")
        return state


def validate_mapping(state: FieldMappingState) -> FieldMappingState:
    try:
        final_mapping = state["final_mapping"]
        validation_results = {"passed": True, "issues": []}
        important_fields = ["è®¢å•å·", "è¿å•å·", "è¿è¾“æ—¥æœŸ"]
        missing_important = [field for field in important_fields if final_mapping.get(field) == "missing"]
        if missing_important:
            validation_results["issues"].append(f"å»ºè®®è¡¥å……é‡è¦å­—æ®µ: {', '.join(missing_important)}")
        if final_mapping.get("è·¯é¡º") != "missing" and final_mapping.get("è¿å•å·") == "missing":
            validation_results["issues"].append("è·¯é¡ºå­—æ®µå­˜åœ¨ä½†ç¼ºå°‘è¿å•å·")
        quantity_fields = ["ä»¶æ•°", "ä½“ç§¯", "é‡é‡"]
        if all(final_mapping.get(field) == "missing" for field in quantity_fields):
            validation_results["issues"].append("ç¼ºå°‘æ‰€æœ‰æ•°é‡ç›¸å…³å­—æ®µ")
        validation_results["passed"] = True
        state["validation_results"] = validation_results
        state["messages"].append({"role": "system", "content": f"æ˜ å°„éªŒè¯å®Œæˆï¼Œé€šè¿‡: {validation_results['passed']}"})
        return state
    except Exception as e:
        state["errors"].append(f"æ˜ å°„éªŒè¯å¤±è´¥: {str(e)}")
        return state


def calculate_confidence_score(state: FieldMappingState) -> FieldMappingState:
    try:
        final_mapping = state["final_mapping"]
        total_fields = len(final_mapping)
        mapped_fields = len([v for v in final_mapping.values() if v != "missing"])
        coverage_rate = mapped_fields / total_fields if total_fields > 0 else 0
        important_fields = ["è®¢å•å·", "è¿å•å·", "è¿è¾“æ—¥æœŸ", "ä»¶æ•°", "ä½“ç§¯", "é‡é‡"]
        other_fields = ["è·¯é¡º", "æ‰¿è¿å•†", "è½¦åž‹", "å‘è´§æ–¹åç§°", "æ”¶è´§æ–¹åç§°", "å•†å“åç§°"]
        important_score = sum(1 for field in important_fields if final_mapping.get(field) != "missing") / len(important_fields)
        other_score = sum(1 for field in other_fields if final_mapping.get(field) != "missing") / len(other_fields)
        validation_score = 1.0
        confidence_score = (0.6 * important_score + 0.3 * other_score + 0.1 * validation_score)
        confidence_score = round(confidence_score * 100, 1)
        state["confidence_score"] = confidence_score
        state["messages"].append({"role": "system", "content": f"å‡†ç¡®çŽ‡è¯„åˆ†å®Œæˆ: {confidence_score}%"})
        return state
    except Exception as e:
        state["errors"].append(f"å‡†ç¡®çŽ‡è¯„åˆ†å¤±è´¥: {str(e)}")
        return state


def generate_output(state: FieldMappingState) -> FieldMappingState:
    try:
        final_mapping = state.get("final_mapping", {})
        validation_results = state.get("validation_results", {"passed": True, "issues": []})
        confidence_score = state.get("confidence_score", 0.0)
        if not final_mapping:
            logger.warning("âš ï¸ final_mappingä¸ºç©ºï¼Œåˆ›å»ºé»˜è®¤æ˜ å°„")
            standard_fields = [
                "è¿è¾“æ—¥æœŸ", "è®¢å•å·", "è·¯é¡º", "æ‰¿è¿å•†", "è¿å•å·", "è½¦åž‹",
                "å‘è´§æ–¹ç¼–å·", "å‘è´§æ–¹åç§°", "æ”¶è´§æ–¹ç¼–ç ", "æ”¶è´§æ–¹åç§°",
                "å•†å“ç¼–ç ", "å•†å“åç§°", "ä»¶æ•°", "ä½“ç§¯", "é‡é‡",
            ]
            final_mapping = {field: "missing" for field in standard_fields}
        analysis = {
            "ä¾æ®": f"åŸºäºŽå­—æ®µåç§°ç‰¹å¾å’Œæ•°æ®ç±»åž‹åˆ†æžï¼Œå…±æ˜ å°„äº† {len([v for v in final_mapping.values() if v != 'missing'])} ä¸ªå­—æ®µ",
            "æé†’": validation_results.get("issues", []),
            "å‡†ç¡®çŽ‡": f"{confidence_score}%",
        }
        output = {"mapping": final_mapping, "analysis": analysis, "confidence": int(confidence_score)}
        state["messages"].append({"role": "assistant", "content": json.dumps(output, ensure_ascii=False)})
        return state
    except Exception as e:
        state["errors"].append(f"ç»“æžœç”Ÿæˆå¤±è´¥: {str(e)}")
        return state


def update_iteration_count(state: FieldMappingState) -> FieldMappingState:
    state["iteration_count"] = state.get("iteration_count", 0) + 1
    state["messages"].append({"role": "system", "content": f"å¼€å§‹ç¬¬ {state['iteration_count']} æ¬¡è¿­ä»£"})
    return state


def should_retry_mapping(state: FieldMappingState) -> str:
    validation_passed = state.get("validation_results", {}).get("passed", True)
    iteration_count = state.get("iteration_count", 0)
    if validation_passed:
        return "generate_output"
    elif iteration_count < 3:
        return "update_iteration"
    else:
        return "generate_output"


def create_field_mapping_graph() -> StateGraph:
    try:
        graph_builder = StateGraph(FieldMappingState)
        graph_builder.add_node("preprocess_fields", preprocess_fields)
        graph_builder.add_node("classify_fields", classify_fields)
        graph_builder.add_node("llm_mapping", llm_map_to_standard_fields)
        graph_builder.add_node("resolve_conflicts", resolve_conflicts_and_missing)
        graph_builder.add_node("validate_mapping", validate_mapping)
        graph_builder.add_node("calculate_confidence", calculate_confidence_score)
        graph_builder.add_node("update_iteration", update_iteration_count)
        graph_builder.add_node("generate_output", generate_output)
        graph_builder.add_edge(START, "preprocess_fields")
        graph_builder.add_edge("preprocess_fields", "classify_fields")
        graph_builder.add_edge("classify_fields", "llm_mapping")
        graph_builder.add_edge("llm_mapping", "resolve_conflicts")
        graph_builder.add_edge("resolve_conflicts", "validate_mapping")
        graph_builder.add_edge("validate_mapping", "calculate_confidence")
        graph_builder.add_conditional_edges(
            "calculate_confidence",
            should_retry_mapping,
            {"update_iteration": "update_iteration", "generate_output": "generate_output"},
        )
        graph_builder.add_edge("update_iteration", "llm_mapping")
        graph_builder.add_edge("generate_output", END)
        return graph_builder.compile(checkpointer=None)
    except Exception as e:
        raise RuntimeError(f"Failed to create field mapping graph: {str(e)}") 