import pandas as pd
import json
import os
from typing import List, Dict, Any, Optional
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

class ExcelProcessor:
    """Excelæ–‡ä»¶å¤„ç†å™¨ï¼Œç”¨äºå°†Excelæ–‡ä»¶è½¬æ¢ä¸ºJSONæ ¼å¼"""
    
    def __init__(self, excel_dir: str = "excel_files"):
        """
        åˆå§‹åŒ–Excelå¤„ç†å™¨
        
        Args:
            excel_dir: Excelæ–‡ä»¶å­˜æ”¾ç›®å½•
        """
        # ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼ŒåŸºäºå½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
        current_file = Path(__file__)
        project_root = current_file.parent.parent.parent  # ä» src/utils/excel_processor.py å›åˆ°é¡¹ç›®æ ¹ç›®å½•
        self.excel_dir = project_root / excel_dir
        self.excel_dir.mkdir(exist_ok=True)
    
    def list_excel_files(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„Excelæ–‡ä»¶"""
        excel_files = []
        if self.excel_dir.exists():
            for file in self.excel_dir.glob("*.xlsx"):
                excel_files.append(file.name)
            for file in self.excel_dir.glob("*.xls"):
                excel_files.append(file.name)
        return excel_files
    
    def read_excel_to_json(self, filename: str, sheet_name: Optional[str] = None) -> Dict[str, Any]:
        """
        è¯»å–Excelæ–‡ä»¶å¹¶è½¬æ¢ä¸ºJSONæ ¼å¼
        
        Args:
            filename: Excelæ–‡ä»¶å
            sheet_name: å·¥ä½œè¡¨åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™è¯»å–æ‰€æœ‰å·¥ä½œè¡¨å¹¶åˆå¹¶
            
        Returns:
            åŒ…å«Excelæ•°æ®çš„å­—å…¸
        """
        try:
            file_path = self.excel_dir / filename
            logger.info(f"å°è¯•è¯»å–æ–‡ä»¶: {file_path}")
            logger.info(f"æ–‡ä»¶æ˜¯å¦å­˜åœ¨: {file_path.exists()}")
            logger.info(f"excel_dirè·¯å¾„: {self.excel_dir}")
            logger.info(f"excel_diræ˜¯å¦å­˜åœ¨: {self.excel_dir.exists()}")
            
            if not file_path.exists():
                raise FileNotFoundError(f"Excelæ–‡ä»¶ä¸å­˜åœ¨: {filename}")
            
            is_xls = file_path.suffix.lower() == '.xls'
            
            # è¯»å–é€»è¾‘ï¼š
            # - æŒ‡å®š sheet_name: åªè¯»è¯¥è¡¨
            # - æœªæŒ‡å®š: è¯»å–æ‰€æœ‰å·¥ä½œè¡¨ï¼Œå„å–å‰100è¡Œååˆå¹¶ï¼Œå¹¶æ·»åŠ  sheet åˆ—
            MAX_ROWS_PER_SHEET = 100
            
            def _normalize_header(df: pd.DataFrame) -> pd.DataFrame:
                """
                å°è¯•è‡ªåŠ¨è¯†åˆ«çœŸå®è¡¨å¤´ï¼š
                - è‹¥å½“å‰åˆ—åå¤§å¤šä¸º Unnamed æˆ–ç©ºï¼Œåˆ™åœ¨å‰10è¡Œä¸­å¯»æ‰¾æœ€å¯èƒ½çš„è¡¨å¤´è¡Œ
                - ä½¿ç”¨è¯¥è¡Œçš„å€¼ä½œä¸ºåˆ—åï¼Œå¹¶ä¸¢å¼ƒå…¶ä¹‹å‰çš„è¡Œ
                - å»é™¤é¦–å°¾ç©ºç™½ã€é‡å¤åˆ—åå»é‡
                """
                try:
                    def _is_unnamed(col: str) -> bool:
                        return isinstance(col, str) and (col.strip() == '' or col.lower().startswith('unnamed'))
                    # è‹¥ç°æœ‰åˆ—å¤§å¤šä¸º Unnamedï¼Œåˆ™å°è¯•åœ¨å‰10è¡Œä¸­å¯»æ‰¾è¡¨å¤´
                    columns = list(df.columns)
                    unnamed_ratio = sum(1 for c in columns if _is_unnamed(str(c))) / max(1, len(columns))
                    if unnamed_ratio < 0.6:
                        # å¤§éƒ¨åˆ†åˆ—åå¯ç”¨ï¼ŒåšåŸºæœ¬æ¸…æ´—å’Œå»é‡
                        cleaned = [str(c).strip() if c is not None else '' for c in columns]
                        # å»é‡
                        seen = {}
                        new_cols = []
                        for c in cleaned:
                            key = c if c != '' else 'åˆ—'
                            if key not in seen:
                                seen[key] = 0
                                new_cols.append(key)
                            else:
                                seen[key] += 1
                                new_cols.append(f"{key}_{seen[key]}")
                        df.columns = new_cols
                        return df
                    # åœ¨å‰10è¡Œå†…å¯»æ‰¾å€™é€‰è¡¨å¤´è¡Œ
                    max_rows_scan = min(10, len(df))
                    best_row = None
                    best_score = -1
                    for r in range(max_rows_scan):
                        row = df.iloc[r]
                        # ç»Ÿè®¡éç©ºå­—ç¬¦ä¸²çš„æ•°é‡ä½œä¸ºåˆ†æ•°
                        non_empty_text = 0
                        for v in row.values:
                            if isinstance(v, str) and v.strip() != '':
                                non_empty_text += 1
                        score = non_empty_text
                        if score > best_score:
                            best_score = score
                            best_row = r
                    if best_row is not None and best_score > 0:
                        # é‡‡ç”¨è¯¥è¡Œä¸ºè¡¨å¤´
                        new_cols = []
                        for v in df.iloc[best_row].values:
                            name = str(v).strip() if v is not None else ''
                            if name == '' or _is_unnamed(name):
                                name = 'åˆ—'
                            new_cols.append(name)
                        # å»é‡
                        seen = {}
                        dedup_cols = []
                        for c in new_cols:
                            if c not in seen:
                                seen[c] = 0
                                dedup_cols.append(c)
                            else:
                                seen[c] += 1
                                dedup_cols.append(f"{c}_{seen[c]}")
                        df = df.iloc[best_row + 1:].reset_index(drop=True)
                        df.columns = dedup_cols
                        logger.info(f"ğŸ”§ è¡¨å¤´å·²é‡ç½®ä¸ºç¬¬{best_row + 1}è¡Œå†…å®¹: {dedup_cols}")
                    else:
                        # æœªæ‰¾åˆ°åˆé€‚çš„è¡¨å¤´ï¼Œä»…åšåŸºæœ¬æ¸…æ´—
                        cleaned = [str(c).strip() if c is not None else '' for c in columns]
                        seen = {}
                        new_cols = []
                        for c in cleaned:
                            key = c if c != '' else 'åˆ—'
                            if key not in seen:
                                seen[key] = 0
                                new_cols.append(key)
                            else:
                                seen[key] += 1
                                new_cols.append(f"{key}_{seen[key]}")
                        df.columns = new_cols
                    return df
                except Exception as _e:
                    logger.warning(f"âš ï¸ è¡¨å¤´è‡ªé€‚åº”å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹è¡¨å¤´: {str(_e)}")
                    return df
            
            def _process_df(df, sheet_label: str) -> (List[Dict[str, Any]], int, int, bool, List[str]):
                original_rows = len(df)
                truncated = False
                if original_rows > MAX_ROWS_PER_SHEET:
                    df = df.head(MAX_ROWS_PER_SHEET)
                    truncated = True
                records = df.to_dict('records')
                processed_records = []
                for record in records:
                    processed_record = {}
                    for key, value in record.items():
                        if pd.isna(value) or value is None:
                            processed_record[key] = None
                        elif isinstance(value, (int, float)):
                            processed_record[key] = value
                        else:
                            processed_record[key] = str(value)
                    # æ·»åŠ æ¥æºsheetä¿¡æ¯
                    processed_record['sheet'] = sheet_label
                    processed_records.append(processed_record)
                return processed_records, original_rows, len(processed_records), truncated, list(df.columns)
            
            sheets_meta = []
            combined_records: List[Dict[str, Any]] = []
            columns_union = set()
            
            if sheet_name:
                try:
                    if is_xls:
                        df = pd.read_excel(file_path, sheet_name=sheet_name, engine='xlrd')
                    else:
                        df = pd.read_excel(file_path, sheet_name=sheet_name)
                except ImportError as ie:
                    if is_xls:
                        raise ImportError("è¯»å– .xls æ–‡ä»¶éœ€è¦ä¾èµ– xlrdï¼Œè¯·å…ˆå®‰è£…: pip install xlrd") from ie
                    raise
                # æ–°å¢ï¼šè¡¨å¤´è‡ªé€‚åº”
                df = _normalize_header(df)
                processed_records, original_rows, data_rows, truncated, cols = _process_df(df, sheet_name)
                combined_records.extend(processed_records)
                columns_union.update(cols)
                sheets_meta.append({
                    'name': sheet_name,
                    'original_rows': original_rows,
                    'data_rows': data_rows,
                    'truncated': truncated
                })
            else:
                # è¯»å–æ‰€æœ‰å·¥ä½œè¡¨
                try:
                    excel_file = pd.ExcelFile(file_path, engine='xlrd') if is_xls else pd.ExcelFile(file_path)
                except ImportError as ie:
                    if is_xls:
                        raise ImportError("è¯»å– .xls æ–‡ä»¶éœ€è¦ä¾èµ– xlrdï¼Œè¯·å…ˆå®‰è£…: pip install xlrd") from ie
                    raise
                total_original_rows = 0
                for sheet in excel_file.sheet_names:
                    df = pd.read_excel(file_path, sheet_name=sheet, engine='xlrd') if is_xls else pd.read_excel(file_path, sheet_name=sheet)
                    # æ–°å¢ï¼šè¡¨å¤´è‡ªé€‚åº”
                    df = _normalize_header(df)
                    processed_records, original_rows, data_rows, truncated, cols = _process_df(df, sheet)
                    combined_records.extend(processed_records)
                    columns_union.update(cols)
                    total_original_rows += original_rows
                    sheets_meta.append({
                        'name': sheet,
                        'original_rows': original_rows,
                        'data_rows': data_rows,
                        'truncated': truncated
                    })
                # è¦†ç›–å•è¡¨è·¯å¾„çš„ original_rows ç»Ÿè®¡æ–¹å¼
                original_rows = total_original_rows
            
            result = {
                "filename": filename,
                "sheet_name": sheet_name or "ALL",
                "total_rows": original_rows if sheet_name else sum(m['original_rows'] for m in sheets_meta),
                "data_rows": len(combined_records),
                "truncated": any(m['truncated'] for m in sheets_meta),
                "sheets": sheets_meta,
                "columns": sorted(list(columns_union)),
                "data": combined_records
            }
            
            if result["truncated"]:
                logger.info(f"æˆåŠŸè¯»å–Excelæ–‡ä»¶: {filename}, å¤šè¡¨åˆå¹¶ï¼ŒåŸå§‹å…±{result['total_rows']}è¡Œï¼Œåˆå¹¶åå–å‰å„{MAX_ROWS_PER_SHEET}è¡Œï¼Œå…±{result['data_rows']}è¡Œ")
            else:
                logger.info(f"æˆåŠŸè¯»å–Excelæ–‡ä»¶: {filename}, å¤šè¡¨åˆå¹¶ï¼ŒåŸå§‹å…±{result['total_rows']}è¡Œï¼Œåˆå¹¶åå…±{result['data_rows']}è¡Œ")
            return result
            
        except Exception as e:
            logger.error(f"è¯»å–Excelæ–‡ä»¶å¤±è´¥: {filename}, é”™è¯¯: {str(e)}")
            raise
    
    def get_excel_info(self, filename: str) -> Dict[str, Any]:
        """
        è·å–Excelæ–‡ä»¶çš„åŸºæœ¬ä¿¡æ¯
        
        Args:
            filename: Excelæ–‡ä»¶å
            
        Returns:
            Excelæ–‡ä»¶ä¿¡æ¯
        """
        try:
            file_path = self.excel_dir / filename
            
            if not file_path.exists():
                raise FileNotFoundError(f"Excelæ–‡ä»¶ä¸å­˜åœ¨: {filename}")
            
            # è¯»å–Excelæ–‡ä»¶ä¿¡æ¯
            excel_file = pd.ExcelFile(file_path)
            
            info = {
                "filename": filename,
                "file_size": file_path.stat().st_size,
                "sheets": excel_file.sheet_names,
                "total_sheets": len(excel_file.sheet_names)
            }
            
            # è·å–ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨çš„åŸºæœ¬ä¿¡æ¯
            if excel_file.sheet_names:
                df = pd.read_excel(file_path, sheet_name=excel_file.sheet_names[0])
                info["first_sheet_rows"] = len(df)
                info["first_sheet_columns"] = list(df.columns)
            
            return info
            
        except Exception as e:
            logger.error(f"è·å–Excelæ–‡ä»¶ä¿¡æ¯å¤±è´¥: {filename}, é”™è¯¯: {str(e)}")
            raise
    
    def save_json_data(self, filename: str, json_data: Dict[str, Any]) -> str:
        """
        å°†JSONæ•°æ®ä¿å­˜åˆ°æ–‡ä»¶
        
        Args:
            filename: ä¿å­˜çš„æ–‡ä»¶å
            json_data: è¦ä¿å­˜çš„JSONæ•°æ®
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        try:
            json_dir = Path("json_data")
            json_dir.mkdir(exist_ok=True)
            
            json_path = json_dir / f"{filename}.json"
            
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"JSONæ•°æ®å·²ä¿å­˜åˆ°: {json_path}")
            return str(json_path)
            
        except Exception as e:
            logger.error(f"ä¿å­˜JSONæ•°æ®å¤±è´¥: {filename}, é”™è¯¯: {str(e)}")
            raise

# åˆ›å»ºå…¨å±€å®ä¾‹
excel_processor = ExcelProcessor() 